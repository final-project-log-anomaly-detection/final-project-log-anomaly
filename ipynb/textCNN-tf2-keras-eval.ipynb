{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":674,"status":"ok","timestamp":1678035637338,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"xcm7t9SLluT1","outputId":"cbc3d94a-aeec-4016-dac8-56a103ffa88a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1683550129693,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"pXZlHDnImrjq"},"outputs":[],"source":["cv = 2\n","sample_ratio = 0.4"]},{"cell_type":"markdown","metadata":{"id":"RnAEJz5FEGZ9"},"source":["# Initialization Section"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47935,"status":"ok","timestamp":1683550177624,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"bkH_BjDlbZV2","outputId":"27204a7f-be8e-45fb-f68c-46ced4a689a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4881,"status":"ok","timestamp":1683550182491,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"7sHKJK_ra9Cf"},"outputs":[],"source":["import tensorflow as tf # tensorflow v2.11.0\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","import re\n","from sklearn.preprocessing import OneHotEncoder\n","from typing import NamedTuple\n","from tqdm.auto import tqdm\n","import pickle\n","# import mmap\n","import gc\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683550182492,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"qvQ5fElk9OG0"},"outputs":[],"source":["class Report:\n","  preserved_names = [\"REPORT\", \"SECTION\"]\n","\n","  def __init__(self, name):\n","    super().__setattr__(\"name\", name)\n","    super().__setattr__(\"_Report__conclusion\", f\"REPORT: {name}\\n{('-' * (8 + len(name)))}\\n\")\n","\n","  def __setattr__(self, name, value):\n","    if name in Report.preserved_names:\n","      raise KeyError(f\"preserved name, {name}\")\n","    \n","    _name = name.replace(\"_\", \" \")\n","    if not hasattr(self, name):\n","      new_line = self.__conclusion + f\"{_name}: {value}\\n\"\n","      super().__setattr__(\"_Report__conclusion\", new_line)\n","    else:\n","      start_index = self.__conclusion.find(f\"{_name}:\")\n","      endln_index = self.__conclusion.find(\"\\n\", start_index)\n","      super().__setattr__(\"_Report__conclusion\", \n","                          self.__conclusion[:start_index] \\\n","                          + f\"{_name}: {value}\" \\\n","                          + self.__conclusion[endln_index:])\n","\n","    super().__setattr__(name, value)\n","\n","  def __str__(self):\n","    return self.__conclusion\n","\n","  def __repr__(self):\n","    return str(self)\n","\n","  def add_section(self, section_name):\n","    new_line = self.__conclusion +\\\n","       f\"\\nSECTION: {section_name.upper()}\\n{('-' * (9 + len(section_name)))}\\n\"\n","    super().__setattr__(\"_Report__conclusion\", new_line)\n","\n","  def save_report(self, path):\n","    with open(path, \"w\") as f:\n","      f.write(str(self))\n","      f.close()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1683550182493,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"mNuRPR41jCHa"},"outputs":[],"source":["pkl_dir = \"/content/drive/MyDrive/Colab Notebooks/preprocessed/\"\n","dataset_prefix = \"BGL.\"\n","prefix = \"withlinetoken.supervised.40-60.\" #+ \"nolabeling.\"\n","\n","cv_prefix = f\"cv{cv}_\" if cv is not None else \"\"\n","\n","template_train_pkl = pkl_dir + dataset_prefix + prefix + f\"{cv_prefix}template.trainset.pkl\"\n","structured_train_pkl = pkl_dir+ dataset_prefix + prefix + f\"{cv_prefix}structured.trainset.pkl\"\n","template_test_pkl = pkl_dir + dataset_prefix + prefix + f\"{cv_prefix}template.testset.pkl\"\n","structured_test_pkl = pkl_dir + dataset_prefix + prefix + f\"{cv_prefix}structured.testset.pkl\"\n","\n","model_name = \"word2vec-google-news-300\"\n","embedder_dir = \"/content/drive/MyDrive/Colab Notebooks/word2Vec/\"\n","fine_tune_files = embedder_dir + dataset_prefix + prefix + cv_prefix + model_name + \".txt\"\n","\n","prep_path = f\"{embedder_dir}{dataset_prefix}{prefix}{cv_prefix}{model_name}.object.pkl\"\n","\n","batch_size = 32\n","min_length = 9\n","\n","seq_lines_len = 5\n","seq_lines_step = 1\n","\n","desample_quantile = 0.9\n","\n","epochs = 5\n","exp_prefix = f\"{seq_lines_len}_lines-downsampling-normal-{sample_ratio}-Q{int(desample_quantile*100):d}.\"\n","reporter = Report(name = f\"{exp_prefix}{prefix}{cv_prefix}report\")\n","\n","training_dir = \"/content/drive/MyDrive/Colab Notebooks/training_model\"\n","if not os.path.exists(training_dir): os.mkdir(training_dir)\n","checkpoint_path = training_dir + \"/{prefix}cp-{epoch:04d}.ckpt\""]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1683550182493,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"QG0mlNEUa9Ch"},"outputs":[],"source":["EmbeddingShape = NamedTuple(\"EmbeddingShape\", [(\"vocab_size\", int), (\"embedding_size\", int)])\n","'''(NamedTuple) class for embedding shape contain vocab_size and embedding_size'''\n","\n","class LogPreprocessor(object):\n","    \"\"\"LogPreprocessor\n","    class for preprocessing data before feed to model\n","    \n","    Args:\n","        record_unknow (bool): whether recording unknown word from preprocessing process or not. Defaults to False.\n","    \"\"\"\n","    \n","    def __init__(self, record_unknow=False) -\u003e None:\n","        if record_unknow: \n","            self.unknow_words = dict()\n","            '''dictionary for counting ocurred unknow word from preprocessing process where keys is word and value is occuring number'''\n","    \n","    @staticmethod\n","    def text_cleansing(text):\n","        \"\"\"(static method) text_cleansing.\n","        method for cleansing log text.\n","        \n","        Args:\n","            text (str): log text to cleansing special character.\n","\n","        Returns:\n","            str: cleansed log text.\n","        \"\"\"\n","        regex_except_token = r'\\B(?!\u003c\\w+\u003e\\B)[^\\w\\s]'\n","        regex_expect_words = r'[^\\w\u003c\u003e]+'\n","        output = re.sub(regex_except_token, '', text)\n","        output = re.sub(regex_expect_words, ' ', output)\n","        return output\n","    \n","    @classmethod\n","    def load_object(cls, path):\n","        with open(path, 'rb') as inp:\n","          obj = pickle.load(inp)\n","          inp.close()\n","\n","        print(f\"vocab size: {obj.embedding_shape.vocab_size}\")\n","        print(f\"embedding size: {obj.embedding_shape.embedding_size}\")\n","\n","        return obj\n","\n","    def save_object(self, path):\n","        with open(path, 'wb') as outp:\n","            pickle.dump(self, outp, -1)\n","\n","    def load_word2vec_format(self, \n","                             file_path, \n","                             unknow_token=None, \n","                             unknow_repr=None,\n","                             chunk_size=0\n","                             ):\n","        \"\"\"(instance method) load_word2vec_format\n","        loading word2vec format file to extract embeadding metrix\n","\n","        Args:\n","            file_path (str): file's path to word2vec format file (.txt)\n","            unknow_token (str, optional): word for represent unknown word (e.g. '\u003cOOV\u003e'). \n","            Defaults to None.\n","            unknow_repr (_ArrayLike, optional): word vector for represent unknown word. \n","            if 'unknow_repr' is not set but 'unknow_token' is set \n","            it will use zero vector as unknow represent vector. Defaults to None.\n","        \"\"\"\n","        # CHUNK_SIZE = mmap.PAGESIZE * chunk_size\n","\n","        with open(file_path, \"r\") as f:\n","            # vec = dict()\n","            first_line = f.readline().split()\n","            vocab_size = int(first_line[0])\n","            embedding_size = int(first_line[-1])\n","            print(f\"vocab size: {vocab_size}\")\n","            print(f\"embedding size: {embedding_size}\")\n","            vec = np.zeros(shape=(vocab_size, embedding_size), dtype=\"float32\")\n","            words_indices = dict()\n","            # f.seek(0)\n","            print(f\"reading file: {file_path}\")\n","            for i, l in tqdm(\n","                enumerate(f.readlines()), \n","                total=vocab_size,\n","                desc=\"build embedding matrix\"\n","                ):\n","                data = list(filter(None, re.split(\" +\", l)))\n","                try:\n","                  vec[i] = np.array(data[1:], dtype=np.float32)\n","                  words_indices[data[0]] = i\n","                except ValueError as e:\n","                  print(f\"ValueError: {data[0]}: {len(data[1:])}\")\n","                  raise e\n","                # if len(vec[data[0]]) != embedding_size:\n","                #     print(\"got unexpected shape at {} ,{}\".format(\n","                #         data[0], len(vec[data[0]]) )\n","                #     )\n","                #     print(\"at line: \", l)\n","            # if chunk_size == 0:\n","            #     for l in f.readlines():\n","            #         data = list(filter(None, re.split(\" +\", l)))\n","            #         vec[data[0]] = np.array(data[1:], dtype=np.float32)\n","            # else:\n","            #     TOTAL_SIZE = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ).size()\n","            #     last_line = \"\"\n","            #     for offset in range(0, TOTAL_SIZE, CHUNK_SIZE):\n","            #         READ_SIZE = CHUNK_SIZE if offset + CHUNK_SIZE \u003c= TOTAL_SIZE else TOTAL_SIZE % CHUNK_SIZE\n","            #         mm = mmap.mmap(f.fileno(), \n","            #                        length=READ_SIZE, \n","            #                        access=mmap.ACCESS_READ, \n","            #                        offset=offset # offset must be multiple of PAGESIZE\n","            #                        )\n","            #         text = last_line + mm.read().decode()\n","            #         lines = text.splitlines()\n","            #         last_line = \"\" if text.endswith('\\n') else lines.pop()\n","\n","            #         for l in lines:\n","            #             try:\n","            #               data = list(filter(None, re.split(\" +\", l)))\n","            #               vec[data[0]] = np.array(data[1:], dtype=np.float32)\n","            #               if len(vec[data[0]]) != embedding_size:\n","            #                   print(\"got unexpected shape at {} ,{}\".format(\n","            #                       data[0], len(vec[data[0]]) )\n","            #                   )\n","            #             except:\n","            #               print(\"error data: {}\" % data)\n","            #               print(\"line (len={}): {}\".format(len(l), l))\n","            #     del text, lines, last_line\n","            #     mm.close()\n","\n","            f.close()\n","                \n","        # embedding_size = int(vec.pop(list(vec.keys())[0])[0])\n","        if unknow_token is not None:\n","            try:\n","                unknow_idx = words_indices[unknow_token]\n","                unknow_vec = vec[unknow_idx]\n","                # unknow_vec = vec[unknow_token]\n","            except AttributeError:\n","                print(f\"there's not '{unknow_token}' in dictionary.\")\n","                if unknow_repr:\n","                    assert len(unknow_repr) == embedding_size, \\\n","                        f\"unknown represent vector must same shape with embedding size (expected {embedding_size}, got {len(unknow_repr)})\"\n","                else:\n","                    unknow_repr = np.zeros(embedding_size, dtype=\"float32\")\n","                # vec[unknow_token] = unknow_repr\n","                vec = np.append(vec, [unknow_repr], axis=0)\n","                words_indices[unknow_token] = vec.shape[0] - 1\n","            \n","            self.unknow_token = unknow_token\n","        else: \n","            self.unknow_token = None\n","        \n","        vocab_size = vec.shape[0]\n","        self.word_vectors = vec\n","        self.words_indices = words_indices\n","        self.embedding_shape = EmbeddingShape(vocab_size, embedding_size)\n","        \n","        gc.collect()\n","    \n","    def indice_encode(self, line):\n","        \"\"\"(instance method) indice_encode.\n","        encoding string words in log line to index number.\n","\n","        Args:\n","            line (_ArrayLike[str]): array of tokenized string line.\n","\n","        Returns:\n","            List[int]: list contains index number.\n","        \"\"\"\n","        record_unknow = hasattr(self, \"unknow_words\")\n","        encoded = list()\n","        \n","        for word in line:\n","            try:\n","                i = self.words_indices[word]\n","            except KeyError:\n","                if record_unknow:\n","                    if word in self.unknow_words.keys(): self.unknow_words[word] += 1\n","                    else: self.unknow_words[word] = 0\n","                \n","                if self.unknow_token is None: continue\n","                else: i = self.words_indices[self.unknow_token]\n","            encoded.append(i)\n","\n","        return np.array(encoded, dtype=np.uint32)\n","    \n","    def insert_new_word(self, word, vector, index=-1):\n","        \"\"\"(instance method) insert_new_word\n","        insert a new word and vector to word dictionary at specified index\n","        \n","        Args:\n","            word (str): new word to insert\n","            vector (_ArrayLike[Float]): new vector to insert\n","            index (int, optional): insert at index if negative index will insert at (vocab_size + index + 1). Defaults to -1.\n","        \"\"\"\n","        if index \u003c 0: index = self.embedding_shape.vocab_size + index + 1\n","        assert word not in self.words_indices.keys(), \\\n","            f\"there is already exist input word in vocab at {self.words_indices[word]} index\"\n","        assert np.ndim(vector) == 1, \\\n","            f\"expect 1 dim vector as a input but got {np.ndim(vector)}.\"\n","        assert len(vector) == self.embedding_shape.embedding_size, \\\n","            f\"insert vector's shape must match to embedding size. (got {np.shape(vector)})\"\n","            \n","        new_word_vectors = np.insert(self.word_vectors, index, vector, axis=0)\n","        new_words_indices = { k: v if v \u003c index else v + 1 for k, v in self.words_indices.items() }\n","        new_words_indices[word] = index\n","        new_embedding_shape = EmbeddingShape(self.embedding_shape.vocab_size + 1, self.embedding_shape.embedding_size)\n","        \n","        self.word_vectors = new_word_vectors\n","        self.words_indices = new_words_indices\n","        self.embedding_shape = new_embedding_shape\n","    \n","    def indice_padding(self, batch, padding_token, padding_size=0):\n","        \"\"\"(instance method) indice_padding.\n","        padding tokenized array in batch by index number of padding_token.\n","        \n","        Args:\n","            batch (_ArrayLike[_ArrayLike[int]]): data batch of array of index number.\n","            padding_token (str): word for represent padding token (e.g. \"\u003cPADDING\u003e\") where padding token must in dictionary.\n","            padding_size (int, optional): size of output. if 'padding_size' is less than longest line in batch it will set to length of longest line is batch. Defaults to 0.\n","\n","        Returns:\n","            NDArray[NDArray[int]]: array of padded batch data.\n","        \"\"\"\n","        padding_idx = self.words_indices[padding_token]\n","        max_length = 0\n","        for inst in batch: \n","            if len(inst) \u003e max_length: max_length = len(inst)\n","        padding_size = max(padding_size, max_length)\n","        \n","        return np.array([ \n","            np.pad(inst, (0, padding_size - len(inst)), constant_values=padding_idx) \\\n","                for inst in tqdm(batch, desc=\"padding\")\n","                ], dtype=np.float32)\n","    \n","    @staticmethod\n","    def build_label(y, classes=None):\n","        \"\"\"(static method) build_label\n","        build labels array using one-hot encoding\n","\n","        Args:\n","            y (_ArrayLike): input labels\n","            classes (_ArrayLike, optional): array contains all classes. if None, it will set to unique label of input. Defaults to None.\n","\n","        Returns:\n","            _Array[_Array[int]]: array of one-hot label where shape is (input_size, num_classes)\n","        \"\"\"\n","        if classes is None:\n","            classes = np.unique(y)\n","            \n","        encoder = OneHotEncoder(categories=[classes], dtype=np.int8)\n","        return encoder.fit_transform(y).toarray()\n","\n","    @staticmethod\n","    def build_sequence(\n","              tokens, \n","              labels, \n","              sequence_length,\n","              step=1, \n","              min_len = 9,\n","              labels_type=\"multi-label\",\n","            ):\n","        assert sequence_length \u003e 0, \"sequence_length must be positive integer more.\"\n","        assert len(tokens) == len(labels), \"len of tokens and labels not equal.\"\n","        sequence_length -= 1\n","        tokens_seq = list() if sequence_length \u003e= 1 else tokens\n","        labels_seq = list()\n","        for i in tqdm(range(0, len(tokens) - sequence_length, step), desc=\"build {} lines sequence\".format(sequence_length + 1)):\n","            if sequence_length \u003e= 1:\n","                seq = np.concatenate(tokens[i:i + sequence_length]).astype(\"uint32\")\n","                if len(seq) \u003c min_len:\n","                  seq = np.pad(seq, (0, min_len - len(seq)), constant_values=0)\n","                tokens_seq.append(seq)\n","            \n","                if labels_type == \"multi-label\":\n","                    labels_seq.append(labels[i:i + sequence_length])\n","                elif labels_type == \"binary\":\n","                    labels_seq.append( \n","                        np.array([0, 1], dtype=\"bool\") \\\n","                        if 1 in labels[i:i + sequence_length] else \\\n","                          np.array([1, 0], dtype=\"bool\")\n","                        )\n","                elif labels_type == \"last\":\n","                    labels_seq.append( \n","                        np.array([0, 1], dtype=\"bool\") \\\n","                        if labels[i + sequence_length - 1] == 1 else \\\n","                        np.array([1, 0], dtype=\"bool\")\n","                    )\n","            else: labels_seq.append(\n","                np.array([0, 1], dtype=\"bool\") \\\n","                if labels[i] == 1 else np.array([1, 0], dtype=\"bool\"))\n","        return tokens_seq, np.array(labels_seq, dtype=np.int8)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36851,"status":"ok","timestamp":1683550219337,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"VJvoWkJUa9Cj","outputId":"cb663878-62f4-45cf-c270-6862267c289c"},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab size: 3006252\n","embedding size: 300\n"]}],"source":["if not os.path.exists(prep_path):\n","    prep = LogPreprocessor()\n","    # extract word2vec format file to embedding matrix and words indice\n","    # change this path for select another embedding method\n","    prep.load_word2vec_format(fine_tune_files)\n","    # insert \"\u003cPAD\u003e\" at first index of embedding matrix with zeros vector\n","    # if padding token is represented as zeros vector, unknown token must be represented\n","    # as non-zeros vector. (that mean if you set unknow_token for load_word2vec_format you\n","    # have to set unknow_repr too or using non-zero vector for represent padding token).\n","    prep.insert_new_word(\"\u003cPAD\u003e\", np.zeros(prep.embedding_shape.embedding_size), index=0)\n","    prep.save_object(prep_path)\n","else: prep = LogPreprocessor.load_object(prep_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1683550219341,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"JZVCXe545A88"},"outputs":[],"source":["# import time\n","# time.sleep(60)\n","\n","# from google.colab import runtime\n","# runtime.unassign()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1683550219342,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"c80jJeoSa9Cl"},"outputs":[],"source":["class TextCNNConfig():\n","    \"\"\"class for containing config of TextCNN model\n","    \"\"\"\n","    \n","    def __init__(self,\n","                num_classes,\n","                vocab_size,\n","                embedding_size, \n","                filter_sizes, \n","                num_filters,\n","                sequence_length=None,\n","                dropout_rate=None,\n","                l2_reg_lambda=0.0,\n","                seed=42,\n","                pretrain_embedding_matrix=None\n","                ) -\u003e None:\n","        self.num_classes = num_classes\n","        self.vocab_size = vocab_size\n","        self.embedding_size = embedding_size\n","        self.filter_sizes = filter_sizes\n","        self.num_filters = num_filters\n","        self.sequence_length = sequence_length\n","        self.dropout_rate = dropout_rate\n","        self.l2_reg_lambda = l2_reg_lambda\n","        self.seed = seed\n","        self.pretrain_embedding_matrix = pretrain_embedding_matrix\n","       \n","    def __repr__(self):\n","        return \"TextCNNConfig:\\n\" +\\\n","            f\"num_classes:      {self.num_classes}\\n\" +\\\n","            f\"vocab_sizes:      {self.vocab_size}\\n\" +\\\n","            f\"embedding_size:   {self.embedding_size}\\n\" +\\\n","            f\"filter_sizes:     {self.filter_sizes}\\n\" +\\\n","            f\"num_filters:      {self.num_filters}\\n\" +\\\n","            f\"sequence_length:  {self.sequence_length}\\n\" +\\\n","            f\"dropout_rate:     {self.dropout_rate}\\n\" +\\\n","            f\"l2_reg_lambda:    {self.l2_reg_lambda}\\n\" +\\\n","            f\"seed:             {self.seed}\\n\" +\\\n","            f\"pretrain_embedding_matrix's shape: {np.shape(self.pretrain_embedding_matrix) if self.pretrain_embedding_matrix is not None else 'None'}\"\n","\n","class TextCNN(object):\n","    \"\"\"CNN model for NLP task presented by Kim Y. in 2014 \" Convolutional Neural Networks for Sentence Classification\"\n","\n","    Args:\n","        num_classes (int): number of classes for classification.\n","        vocab_size (int): number of known vocabularies. if pretrain_embedding_matrix is set, vocab_size must rely on pretrain_embedding_matrix' shape.\n","        embedding_size (int): length of embedding vector. if pretrain_embedding_matrix is set, embedding_size must rely on pretrain_embedding_matrix' shape.\n","        filter_sizes (_ArrayLike[int]): array contain convolution filter height.\n","        num_filters (int): number of each filter size.\n","        sequence_length (int, optional): length of input sequence. if None, input can be any length. Defualts is None.\n","        dropout_rate (float, optionanl): dropout rate for dropout layer. if None, model will not include dropout layer. Defualts is None.\n","        l2_reg_lambda (float, optional): L2 regularization factor for dense layer. Defaults is 0.0\n","        seed (int, optional): random seed. Defaults is 42.\n","        pretrain_embedding_matrix (_2DArray[float]): pretained embedding vectors. if None, embedding matrix will be initialized by random and it will be trained while trainning else it will be not trainable. Defaults is None\n","    \"\"\"\n","    \n","    def __init__(self,\n","                num_classes,\n","                vocab_size,\n","                embedding_size, \n","                filter_sizes, \n","                num_filters,\n","                sequence_length=None,\n","                dropout_rate=None,\n","                l2_reg_lambda=0.0,\n","                seed=42,\n","                pretrain_embedding_matrix=None,\n","                output_activation=\"softmax\"\n","        ) -\u003e None:\n","        \n","        self.config = TextCNNConfig(\n","            num_classes,\n","            vocab_size,\n","            embedding_size, \n","            filter_sizes, \n","            num_filters,\n","            sequence_length,\n","            dropout_rate,\n","            l2_reg_lambda,\n","            seed,\n","            pretrain_embedding_matrix\n","        )\n","        \n","        # set global random seed\n","        if seed is not None: tf.random.set_seed(seed)\n","        # define input layer where input shape is (batch_size, sequence_length)\n","        # input data is 2d-array which each value is index of jth word's word vector \n","        # in embedding metrix of ith line.\n","        input_word_idx = tf.keras.layers.Input(\n","            shape=(sequence_length,),\n","            dtype=tf.dtypes.int32,\n","            name=\"input-word-idx-layer\"\n","        )\n","        \n","        # if pretrain_embedding_matrix is not defined, using random uniform for \n","        # initailize embedding matrix\n","        embed_trainable = pretrain_embedding_matrix is None\n","        if pretrain_embedding_matrix is None:\n","            embed_initializers = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n","        else:\n","            pretrain_embedding_matrix = np.array(pretrain_embedding_matrix)\n","            assert (vocab_size, embedding_size) == pretrain_embedding_matrix.shape, \\\n","                f\"shape of embedding_matrix must match to vocab_size and embedding_size (expect {(vocab_size, embedding_size)}, got {pretrain_embedding_matrix.shape}).\"\n","            embed_initializers = tf.keras.initializers.Constant(pretrain_embedding_matrix)\n","        # the embedding layer will defaultly use GPU memory\n","        # so to avoid error while trainning from optimizer which not support GPU\n","        # using tf.device(\"cpu:0\") to place embedding matrix on CPU memory\n","        with tf.name_scope(\"embedding\"), tf.device(\"cpu:0\"):\n","            # define embedding layer where in put is array of index of word's vector in\n","            # embedding matrix. the output's shape is \n","            # (batch_size, sequence_length, embedding_size)\n","            embed = tf.keras.layers.Embedding(\n","                input_dim=vocab_size,\n","                output_dim=embedding_size,\n","                embeddings_initializer=embed_initializers,\n","                input_length=sequence_length,\n","                trainable=embed_trainable,\n","                name=\"embedding-layer\"\n","            )(input_word_idx)\n","            # define reshape layer for expand dimention of output from embedding layer to\n","            # (batch_size, sequence_length, embedding_size, 1) for suit to input of\n","            # 2d convolution layer which require input shape as \n","            # (batch_size, weight, height, channel)\n","            expand_dim_embed = tf.keras.layers.Reshape(\n","                target_shape=(-1, embedding_size, 1),\n","                name=\"reshape-expand-dim-layer\"\n","                )(embed)\n","        \n","        features = list() # list for contain output of each pooling layer\n","        with tf.name_scope(\"convolution\"), tf.device(\"gpu:0\"):\n","            # for loop to define each convolution layer of each size of the filter\n","            for size in filter_sizes:\n","                # define 2d convolution layer of n(num_filters) filters with kernel size\n","                # is (size, embedding_size) strides by (1, 1) and not use padding.\n","                # using ReLu as activation function also use bias. output's shape of this\n","                # layer is (batch_size, sequence_length - size + 1, 1, num_filters)\n","                conv = tf.keras.layers.Conv2D(\n","                    filters=num_filters,\n","                    kernel_size=(size, embedding_size),\n","                    strides=(1, 1),\n","                    padding=\"valid\",\n","                    activation=\"relu\",\n","                    use_bias=True,\n","                    kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1),\n","                    bias_initializer=tf.keras.initializers.Constant(0.1),\n","                    name=f\"conv-{size}_{embedding_size}-layer\"\n","                )(expand_dim_embed)\n","                # define reshape layer for squeeze dimention of output from convolution layer \n","                # to (batch_size, feature_map_height, num_filters)\n","                reshape = tf.keras.layers.Reshape(target_shape=(-1, num_filters), name=f\"reshape-squeeze-{size}_{embedding_size}-layer\")(conv)\n","                # define max-over-time pooling layer for extract max value from each \n","                # feature map from convolution layers. where output is vector with \n","                # shape of (batch_size, 1, num_filters)\n","                pooling = tf.keras.layers.GlobalMaxPool1D(name=f\"global-max-pooling-{size}_{embedding_size}-layer\")(reshape)\n","                # define flatten layer for shape output of pooling layer to \n","                # (batch_size, num_filters)\n","                flatten_pooling = tf.keras.layers.Flatten(name=f\"flatten-pooling-{size}_{embedding_size}-layer\")(pooling)\n","                features.append(flatten_pooling)\n","        \n","        # define concatenate layer to concate all features for convolution \u0026 pooling \n","        # process. where output shape is (batch_size, num_filters * len(filter_sizes))\n","        concat = tf.keras.layers.Concatenate(name=\"concatenate-layer\")(features)\n","        \n","        # if dropout_rate is None, model will not include dropout layer\n","        if dropout_rate is not None:\n","            with tf.name_scope(\"dropout\"):\n","                # define dropout layer with specified dropout_rate\n","                dropout = tf.keras.layers.Dropout(rate=dropout_rate)(concat)\n","                fc_input = dropout\n","        else: fc_input = concat\n","        \n","        with tf.name_scope(\"fully-connected\"), tf.device(\"gpu:0\"):\n","            # define output layer (fully connected layer) using Softmax as activation\n","            # and L2 as regularization method. where output is propability to be each \n","            # class with output shape is (batch_size, num_classes).\n","            output = tf.keras.layers.Dense(\n","                units=num_classes,\n","                activation=\"softmax\",\n","                use_bias=True,\n","                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n","                bias_initializer=tf.keras.initializers.Constant(0.1),\n","                kernel_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n","                bias_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n","                name=\"output-layer\"\n","                )(fc_input)\n","        \n","        # define model with all defined sequent layers\n","        self.model = tf.keras.Model(inputs=input_word_idx, outputs=output)\n","        "]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1683550219342,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"68hJ4FRhERaX"},"outputs":[],"source":["def gen_series(X, y):\n","  assert len(X) == len(y), \"length of X and y must equal.\"\n","  for i in range(len(X)):\n","    yield X[i], y[i]\n","\n","def gen_predictor(X):\n","  for i in range(len(X)):\n","    yield X[i]"]},{"cell_type":"markdown","metadata":{"id":"75uVdjzLDBGU"},"source":["# Training Evaluation Section"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":64055,"status":"ok","timestamp":1683550283380,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"QQ8xCYaNnMQU","outputId":"e4c5657c-3888-488a-abf5-40ea458c6d0c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d32c571bd8bc4218bdadaa0f921d1264","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3142329 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5e90cc1c6464be99177c388092b33cf","version_major":2,"version_minor":0},"text/plain":["build 5 lines sequence:   0%|          | 0/3142325 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["prep_train_struc_log = pd.read_pickle(structured_train_pkl)\n","\n","prep_train_struc_log[\"Token_Indice_encoded\"] =\\\n","    prep_train_struc_log.Token.progress_apply(\n","        prep.indice_encode\n","        )\n","\n","train_tokens, train_labels = LogPreprocessor.build_sequence(\n","    prep_train_struc_log[\"Token_Indice_encoded\"].values,\n","    prep_train_struc_log[\"Label\"].values,\n","    sequence_length = seq_lines_len, step= seq_lines_step,\n","    labels_type = \"last\",\n","    min_len = min_length,\n","    )\n","\n","del prep_train_struc_log\n","gc.collect()\n","\n","train_X = tf.data.Dataset.from_generator(\n","    lambda: gen_predictor(train_tokens),\n","    output_signature=tf.TensorSpec(\n","                      shape=(None, ), \n","                      dtype=\"uint32\",\n","                      name=\"features\"\n","                      )\n","    )\n","\n","train_X = train_X.apply(\n","    tf.data.experimental.assert_cardinality(len(train_tokens))\n","    )"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683550283381,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"IjQhb4ifqMWZ"},"outputs":[],"source":["train_batch = train_X.padded_batch(batch_size)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8675,"status":"ok","timestamp":1683550292047,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"xObZSA6CDJkT"},"outputs":[],"source":["textcnn = TextCNN(\n","                num_classes=2,\n","                vocab_size=prep.embedding_shape.vocab_size,\n","                embedding_size=prep.embedding_shape.embedding_size,\n","                filter_sizes=[2, 3, 5, 7, 9],\n","                num_filters=3,\n","                sequence_length=None,\n","                dropout_rate=0.5,\n","                l2_reg_lambda=0.01,\n","                seed=42,\n","                pretrain_embedding_matrix=prep.word_vectors)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25499,"status":"ok","timestamp":1683550321453,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"TnLBHjPLDWjp","outputId":"5b943404-fda3-4936-a9d3-1e83e9123de1"},"outputs":[{"data":{"text/plain":["\u003ctensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f75ce8d8820\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["textcnn.model.load_weights(checkpoint_path.format(prefix=f\"{exp_prefix}{prefix}{cv_prefix}\", epoch=epochs))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"OIK7x-9CoC3B"},"outputs":[{"name":"stdout","output_type":"stream","text":["98198/98198 [==============================] - 795s 8ms/step\n"]}],"source":["eval_pred = textcnn.model.predict(x=train_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6Y8CiZyhoIiQ"},"outputs":[{"data":{"text/plain":["182"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["del train_tokens, textcnn\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aeRxbnCSoS6w"},"outputs":[],"source":["eval_pred_df = pd.DataFrame({\n","    \"label\": train_labels.argmax(axis=1),\n","    \"normal_prob\": eval_pred.T[0],\n","    \"abnormal_prob\": eval_pred.T[1]\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fulTtJgMoey2"},"outputs":[{"data":{"text/plain":["array([[ 170457, 2717771],\n","       [      0,  254097]], dtype=int32)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix = tf.math.confusion_matrix(\n","    train_labels.argmax(axis=1), \n","    eval_pred.argmax(axis=1)).numpy()\n","confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"F9Wo_9zrojYv"},"outputs":[{"name":"stdout","output_type":"stream","text":["PRE: 1.0, REC 0.05901784762144817\n","ACC: 0.1351082399178952\n"]}],"source":["positive, negative = 0, 1\n","tp = confusion_matrix[positive][positive]\n","fp = confusion_matrix[negative][positive]\n","tn = confusion_matrix[negative][negative]\n","fn = confusion_matrix[positive][negative]\n","\n","pre = tp / (tp + fp) \n","rec = tp / (tp + fn)\n","print(f\"PRE: {pre}, REC {rec}\")\n","\n","acc = (tp + tn) / confusion_matrix.sum()\n","print(f\"ACC: {acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0w-ZNtOBomlp"},"outputs":[],"source":["reporter.add_section(f\"training result\")\n","reporter.positive = positive\n","reporter.negative = negative\n","reporter.TP = tp\n","reporter.FP = fp\n","reporter.TN = tn\n","reporter.FN = fn\n","reporter.precision_0 = pre\n","reporter.recall_0 = rec\n","reporter.accuracy_0 = acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i1wtzB5doqYO"},"outputs":[{"name":"stdout","output_type":"stream","text":["PRE: 0.08550076921316828, REC 1.0\n","ACC: 0.1351082399178952\n"]}],"source":["positive, negative = 1, 0\n","tp = confusion_matrix[positive][positive]\n","fp = confusion_matrix[negative][positive]\n","tn = confusion_matrix[negative][negative]\n","fn = confusion_matrix[positive][negative]\n","\n","pre = tp / (tp + fp) \n","rec = tp / (tp + fn)\n","print(f\"PRE: {pre}, REC {rec}\")\n","\n","acc = (tp + tn) / confusion_matrix.sum()\n","print(f\"ACC: {acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iw2HXHODot_m"},"outputs":[],"source":["reporter.add_section(f\"training result (reverse)\")\n","reporter.precision_1 = pre\n","reporter.recall_1 = rec\n","reporter.accuracy_1 = acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IGgjh17Mow8F"},"outputs":[{"data":{"text/plain":["REPORT: 5_lines-downsampling-normal-0.4-Q90.withlinetoken.supervised.40-60.cv2_report\n","-------------------------------------------------------------------------------------\n","\n","SECTION: TRAINING RESULT\n","------------------------\n","positive: 0\n","negative: 1\n","TP: 170457\n","FP: 0\n","TN: 254097\n","FN: 2717771\n","precision 0: 1.0\n","recall 0: 0.05901784762144817\n","accuracy 0: 0.1351082399178952\n","\n","SECTION: TRAINING RESULT (REVERSE)\n","----------------------------------\n","precision 1: 0.08550076921316828\n","recall 1: 1.0\n","accuracy 1: 0.1351082399178952"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["reporter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Y-5Nq_qaYO13"},"outputs":[],"source":["from datetime import datetime\n","\n","time_prefix = datetime.today().strftime(\"%d-%m-%y_%H-%M.\")\n","\n","report_dir = \"/content/drive/MyDrive/Colab Notebooks/report/\"\n","if not os.path.exists(report_dir):\n","  os.mkdir(report_dir)\n","\n","reporter.save_report(report_dir + time_prefix + reporter.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pumXxWLCYDwv"},"outputs":[],"source":["import time\n","time.sleep(60)\n","\n","from google.colab import runtime\n","runtime.unassign()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Om6JNJgIMKwQ"},"outputs":[],"source":["prep_test_struc_log = pd.read_pickle(structured_test_pkl)[[\"Token\", \"Label\"]]\n","# prep_test_templ_log = pd.read_pickle(template_test_pkl)\n","# prep_test_struc_log = define_dtype(prep_test_struc_log, dtype_struc_log_map)\n","# prep_test_templ_log = define_dtype(prep_test_templ_log, dtype_templ_log_map)\n","\n","# print(\"load testset: strunctured, template = ({}, {})\"\n","#           .format( len(prep_test_struc_log), len(prep_test_templ_log) )\n","#     )\n","\n","token_indice_encoded = prep_test_struc_log.Token.progress_apply(\n","    prep.indice_encode\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D58yWfEn5ule"},"outputs":[],"source":["prep_test_struc_log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j210gQ6jBARB"},"outputs":[],"source":["test_tokens, test_labels = LogPreprocessor.build_sequence(\n","    token_indice_encoded.values,\n","    prep_test_struc_log[\"Label\"].values,\n","    sequence_length = seq_lines_len, step= seq_lines_step,\n","    labels_type = \"last\",\n","    min_len = min_length,\n","    )\n","# test_label = LogPreprocessor.build_label(prep_test_struc_log[\"Label\"].to_numpy().reshape((-1, 1)), classes=[0, 1]) \n","# padd_test_input = prep.indice_padding(test_tokens, \"\u003cPAD\u003e\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d52WG9zVr06N"},"outputs":[],"source":["reporter.add_section(\"testing set\")\n","reporter.testing_num_lines = len(prep_test_struc_log)\n","# reporter.testing_num_templates = len(prep_test_templ_log)\n","reporter.testing_size = len(test_tokens)\n","reporter.testing_abnormal = test_labels.argmax(axis=1).sum()\n","reporter.testing_normal = reporter.testing_size - reporter.testing_abnormal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91SKXBXNz0xc"},"outputs":[],"source":["del prep_test_struc_log, token_indice_encoded #, prep_test_templ_log\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptqU7PjLX8Vo"},"outputs":[],"source":["test_X = tf.data.Dataset.from_generator(\n","    lambda: gen_predictor(test_tokens),\n","    output_signature=tf.TensorSpec(\n","                      shape=(None, ), \n","                      dtype=\"uint32\",\n","                      name=\"features\"\n","                      )\n","    )\n","\n","test_X = test_X.apply(\n","    tf.data.experimental.assert_cardinality(len(test_tokens))\n","    )\n","test_batch = test_X.padded_batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a8YwGM-EnE_"},"outputs":[],"source":["textcnn = TextCNN(\n","                num_classes=2,\n","                vocab_size=prep.embedding_shape.vocab_size,\n","                embedding_size=prep.embedding_shape.embedding_size,\n","                filter_sizes=[2, 3, 5, 7, 9],\n","                num_filters=3,\n","                sequence_length=None,\n","                dropout_rate=0.5,\n","                l2_reg_lambda=0.01,\n","                seed=42,\n","                pretrain_embedding_matrix=prep.word_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgjAuK--Tee_"},"outputs":[],"source":["textcnn.model.load_weights(checkpoint_path.format(prefix=f\"{exp_prefix}{prefix}{cv_prefix}\", epoch=epochs))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNl0se_ba9Co"},"outputs":[],"source":["pred = textcnn.model.predict(x=test_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwF5R244a9Cl"},"outputs":[],"source":["del test_tokens, textcnn\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAfOIkjydgCs"},"outputs":[],"source":["pred_df = pd.DataFrame({\n","    \"label\": test_labels.argmax(axis=1),\n","    \"normal_prob\": pred.T[0],\n","    \"abnormal_prob\": pred.T[1]\n","    })\n","# prep_test_struc_log[\"Predict\"] = pred.argmax(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U22WEVl-6Kbn"},"outputs":[],"source":["pred_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Av5Tdb-DSB78"},"outputs":[],"source":["pred_df[pred_df.label == 1][[\"normal_prob\", \"abnormal_prob\"]].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm2SySbA41sf"},"outputs":[],"source":["confusion_matrix = tf.math.confusion_matrix(test_labels.argmax(axis=1), pred.argmax(axis=1)).numpy()\n","confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AIcSn-b58gY"},"outputs":[],"source":["positive, negative = 0, 1\n","tp = confusion_matrix[positive][positive]\n","fp = confusion_matrix[negative][positive]\n","tn = confusion_matrix[negative][negative]\n","fn = confusion_matrix[positive][negative]\n","\n","pre = tp / (tp + fp) \n","rec = tp / (tp + fn)\n","print(f\"PRE: {pre}, REC {rec}\")\n","\n","acc = (tp + tn) / confusion_matrix.sum()\n","print(f\"ACC: {acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69rZ0Ey0uzQl"},"outputs":[],"source":["reporter.add_section(f\"testing result\")\n","reporter.positive = positive\n","reporter.negative = negative\n","reporter.TP = tp\n","reporter.FP = fp\n","reporter.TN = tn\n","reporter.FN = fn\n","reporter.precision_0 = pre\n","reporter.recall_0 = rec\n","reporter.accuracy_0 = acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCBHjuhpzxrw"},"outputs":[],"source":["positive, negative = 1, 0\n","tp = confusion_matrix[positive][positive]\n","fp = confusion_matrix[negative][positive]\n","tn = confusion_matrix[negative][negative]\n","fn = confusion_matrix[positive][negative]\n","\n","pre = tp / (tp + fp) \n","rec = tp / (tp + fn)\n","print(f\"PRE: {pre}, REC {rec}\")\n","\n","acc = (tp + tn) / confusion_matrix.sum()\n","print(f\"ACC: {acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GotcEkEBzjQZ"},"outputs":[],"source":["reporter.add_section(f\"testing result (reverse)\")\n","reporter.precision_1 = pre\n","reporter.recall_1 = rec\n","reporter.accuracy_1 = acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kglzPfzu0a8L"},"outputs":[],"source":["reporter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dN_sVNgnYTKH"},"outputs":[],"source":["from datetime import datetime\n","\n","time_prefix = datetime.today().strftime(\"%d-%m-%y_%H-%M.\")\n","\n","report_dir = \"/content/drive/MyDrive/Colab Notebooks/report/\"\n","if not os.path.exists(report_dir):\n","  os.mkdir(report_dir)\n","\n","reporter.save_report(report_dir + time_prefix + reporter.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyiCsfoNYXA5"},"outputs":[],"source":["import time\n","time.sleep(60)\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"colab":{"collapsed_sections":["RnAEJz5FEGZ9","LftfiXvNEtnQ"],"machine_shape":"hm","name":"","version":""},"gpuClass":"premium","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0e7d12f8cec341508725e7547fa357eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1954673009ce4263ada911fc367ea744":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b1d8f8d6e5f4107980f901f41fa24eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c3c1832f2e74b1b82b2b7e13f31e704":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"412aa472a2f947fa92e79c4a43e9d67d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44c5eb59694f4b60974b942eef659498":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb09cb68517848ecb8c99fe0773fa895","placeholder":"​","style":"IPY_MODEL_5f53455cbd8e4caabca55b455d4dde52","value":" 3142325/3142325 [00:24\u0026lt;00:00, 128397.70it/s]"}},"55be91ba61e640e18e472fbb871f2868":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c3c1832f2e74b1b82b2b7e13f31e704","placeholder":"​","style":"IPY_MODEL_d577abf624f14820b2f245023ee0acdc","value":"build 5 lines sequence: 100%"}},"5bde2bed72944af8b8fa3070e2bf6e00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f53455cbd8e4caabca55b455d4dde52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65c4d88ab1ec412fbd46354e789f3435":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83a3a8711d7f49598a33c50226c8c8b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"999addf3241e4183b1dc0a849ce7318a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a528e7de6ec34cefb544661b66df8882":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83a3a8711d7f49598a33c50226c8c8b2","placeholder":"​","style":"IPY_MODEL_b330ab3ede6c41d1a08e5826d93ceeec","value":" 3142329/3142329 [00:11\u0026lt;00:00, 225398.44it/s]"}},"b330ab3ede6c41d1a08e5826d93ceeec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcc80ef39dbf4ee88f1ba040309a5a6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b1d8f8d6e5f4107980f901f41fa24eb","max":3142325,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1954673009ce4263ada911fc367ea744","value":3142325}},"c5e90cc1c6464be99177c388092b33cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55be91ba61e640e18e472fbb871f2868","IPY_MODEL_bcc80ef39dbf4ee88f1ba040309a5a6f","IPY_MODEL_44c5eb59694f4b60974b942eef659498"],"layout":"IPY_MODEL_5bde2bed72944af8b8fa3070e2bf6e00"}},"cd204d39e9a34e9fba47191e07229c71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb2d9d6b39174ae8bcf76caeb4e5f3d1","placeholder":"​","style":"IPY_MODEL_65c4d88ab1ec412fbd46354e789f3435","value":"100%"}},"d32c571bd8bc4218bdadaa0f921d1264":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd204d39e9a34e9fba47191e07229c71","IPY_MODEL_d49310e5c8574e079024d03125dde129","IPY_MODEL_a528e7de6ec34cefb544661b66df8882"],"layout":"IPY_MODEL_999addf3241e4183b1dc0a849ce7318a"}},"d49310e5c8574e079024d03125dde129":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_412aa472a2f947fa92e79c4a43e9d67d","max":3142329,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e7d12f8cec341508725e7547fa357eb","value":3142329}},"d577abf624f14820b2f245023ee0acdc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb2d9d6b39174ae8bcf76caeb4e5f3d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb09cb68517848ecb8c99fe0773fa895":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}