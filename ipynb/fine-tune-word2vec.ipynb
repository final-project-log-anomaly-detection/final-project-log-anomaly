{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_18486/93258942.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "split_date = struct_log[struct_log.Label == 1].Date.values[0]\n",
    "\n",
    "trainset = struct_log[struct_log.Date < split_date]\n",
    "testset = struct_log[struct_log.Date >= split_date]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n",
    "\n",
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_18486/1848025451.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_18486/1848025451.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainset[\"Token\"] = trainset.EventId.map(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "trainset[\"Token\"] = trainset.EventId.map(\n",
    "    lambda id: template_log_train[template_log_train.EventId == id].\n",
    "    EventTemplateIdent_token.values[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "test_ratio = 0.4\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "\n",
    "trainset, testset = train_test_split(struct_log, test_size=0.4, random_state=seed, shuffle=False)\n",
    "trainset = trainset[trainset[\"Label\"] == 0]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n",
    "\n",
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "trainset[\"Token\"] = trainset.EventId.map(\n",
    "    lambda id: template_log_train[template_log_train.EventId == id].\n",
    "    EventTemplateIdent_token.values[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/anomaly_detection/lib/python3.8/site-packages/gensim/models/base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Download \"GoogleNews-vectors-negative300.bin\" from:\n",
    "# https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"../models/GoogleNews-vectors-negative300.bin\",\n",
    "    binary = True\n",
    "    )\n",
    "\n",
    "embedder = Word2Vec(size=300, min_count=1)\n",
    "embedder.build_vocab(token_train_list)\n",
    "total_examples = embedder.corpus_count\n",
    "embedder.build_vocab([list(model.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.intersect_word2vec_format(\"../models/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_18486/1153265867.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  embedder.train(token_train_list, total_examples=total_examples, epochs=embedder.iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(678, 735)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.train(token_train_list, total_examples=total_examples, epochs=embedder.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedder\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49msave_word2vec_format(\u001b[39m\"\u001b[39;49m\u001b[39m../models/BGL-fine-tune-embedder.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/anomaly_detection/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1452\u001b[0m, in \u001b[0;36mWord2VecKeyedVectors.save_word2vec_format\u001b[0;34m(self, fname, fvocab, binary, total_vec)\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[39m\"\"\"Store the input-hidden weight matrix in the same format used by the original\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \u001b[39mC word2vec-tool, for compatibility.\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1449\u001b[0m \n\u001b[1;32m   1450\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[39m# from gensim.models.word2vec import save_word2vec_format\u001b[39;00m\n\u001b[0;32m-> 1452\u001b[0m _save_word2vec_format(\n\u001b[1;32m   1453\u001b[0m     fname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, total_vec\u001b[39m=\u001b[39;49mtotal_vec)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/anomaly_detection/lib/python3.8/site-packages/gensim/models/utils_any2vec.py:291\u001b[0m, in \u001b[0;36m_save_word2vec_format\u001b[0;34m(fname, vocab, vectors, fvocab, binary, total_vec)\u001b[0m\n\u001b[1;32m    289\u001b[0m     fout\u001b[39m.\u001b[39mwrite(utils\u001b[39m.\u001b[39mto_utf8(word) \u001b[39m+\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m row\u001b[39m.\u001b[39mtostring())\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     fout\u001b[39m.\u001b[39;49mwrite(utils\u001b[39m.\u001b[39;49mto_utf8(\u001b[39m\"\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m (word, \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mrepr\u001b[39;49m(val) \u001b[39mfor\u001b[39;49;00m val \u001b[39min\u001b[39;49;00m row))))\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embedder.wv.save_word2vec_format(\"../models/BGL-fine-tune-embedder.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('anomaly_detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 15:49:01) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb13736d41bd87c4a6da44de2a53cc3b2c3a6ec16bd51d52b3766a818a83a02f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
