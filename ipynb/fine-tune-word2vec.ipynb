{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/1228816633.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#######\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"../Drain_result/thunderbird_small.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"../Drain_result/thunderbird_small.log_templates.csv\")\n",
    "\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "split_date = struct_log[struct_log.Label == 1].Date.values[0]\n",
    "\n",
    "trainset = struct_log[struct_log.Date < split_date]\n",
    "testset = struct_log[struct_log.Date >= split_date] \n",
    "eventid_train = trainset.EventId.unique() \n",
    "eventid_test = testset.EventId.unique() \n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/2076871793.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
      "Mapping ID & token: 100%|██████████| 15/15 [00:00<00:00, 86659.17it/s]\n",
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/2076871793.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainset[\"Token\"] = trainset.EventId.map(lambda id: map_token_train[id])\n"
     ]
    }
   ],
   "source": [
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "map_token_train = { row[0]: row[1] \\\n",
    "    for row in tqdm(\n",
    "        template_log_train[[\"EventId\", \"EventTemplateIdent_token\"]].values,\n",
    "        desc=\"Mapping ID & token\"\n",
    "        ) }\n",
    "trainset[\"Token\"] = trainset.EventId.map(lambda id: map_token_train[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# clean special character\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "test_ratio = 0.4\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "\n",
    "trainset, testset = train_test_split(struct_log, test_size=0.4, random_state=seed, shuffle=False)\n",
    "trainset = trainset[trainset[\"Label\"] == 0]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "map_token_train = { row[0]: row[1] \\\n",
    "    for row in tqdm(\n",
    "        template_log_train[[\"EventId\", \"EventTemplateIdent_token\"]].values,\n",
    "        desc=\"Mapping ID & token\"\n",
    "        ) }\n",
    "trainset[\"Token\"] = trainset.EventId.map(lambda id: map_token_train[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m embedder\u001b[39m.\u001b[39mbuild_vocab(token_train_list)\n\u001b[1;32m     15\u001b[0m total_examples \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39mcorpus_count\n\u001b[0;32m---> 16\u001b[0m embedder\u001b[39m.\u001b[39mbuild_vocab([\u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39mkeys())], update\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/gensim/models/keyedvectors.py:735\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "# Download \"GoogleNews-vectors-negative300.bin\" from:\n",
    "# https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"../google_news/GoogleNews-vectors-negative300.bin\",\n",
    "    binary = True\n",
    "    )\n",
    "\n",
    "embedder = Word2Vec(vector_size=300, min_count=1)\n",
    "# embedder.build_vocab(token_train_list)\n",
    "# total_examples = embedder.corpus_count\n",
    "# embedder.build_vocab([list(model.key_to_index.keys())], update=True)\n",
    "# embedder = Word2Vec(size=300, min_count=1)\n",
    "embedder.build_vocab(token_train_list)\n",
    "total_examples = embedder.corpus_count\n",
    "embedder.build_vocab([list(model.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.wv.vectors_lockf = np.ones(len(embedder.wv), dtype=np.float32)\n",
    "embedder.wv.intersect_word2vec_format(\"../google_news/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/1153265867.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  embedder.train(token_train_list, total_examples=total_examples, epochs=embedder.iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(303, 735)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.train(token_train_list, total_examples=total_examples, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.wv.save_word2vec_format(\"../thunderbird_10M-fine-tune-embedder.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce4081ce2b9ab9ecfe855a4e9c840ae25394b5d73782b2027cad0c1ddfa0aa02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
