{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/1228816633.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "split_date = struct_log[struct_log.Label == 1].Date.values[0]\n",
    "\n",
    "trainset = struct_log[struct_log.Date < split_date]\n",
    "testset = struct_log[struct_log.Date >= split_date]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/2076871793.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
      "Mapping ID & token: 100%|██████████| 15/15 [00:00<00:00, 86659.17it/s]\n",
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/2076871793.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainset[\"Token\"] = trainset.EventId.map(lambda id: map_token_train[id])\n"
     ]
    }
   ],
   "source": [
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "map_token_train = { row[0]: row[1] \\\n",
    "    for row in tqdm(\n",
    "        template_log_train[[\"EventId\", \"EventTemplateIdent_token\"]].values,\n",
    "        desc=\"Mapping ID & token\"\n",
    "        ) }\n",
    "trainset[\"Token\"] = trainset.EventId.map(lambda id: map_token_train[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "test_ratio = 0.4\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "\n",
    "trainset, testset = train_test_split(struct_log, test_size=0.4, random_state=seed, shuffle=False)\n",
    "trainset = trainset[trainset[\"Label\"] == 0]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "map_token_train = { row[0]: row[1] \\\n",
    "    for row in tqdm(\n",
    "        template_log_train[[\"EventId\", \"EventTemplateIdent_token\"]].values,\n",
    "        desc=\"Mapping ID & token\"\n",
    "        ) }\n",
    "trainset[\"Token\"] = trainset.EventId.map(lambda id: map_token_train[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/anomaly_detection/lib/python3.8/site-packages/gensim/models/base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Download \"GoogleNews-vectors-negative300.bin\" from:\n",
    "# https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"../models/GoogleNews-vectors-negative300.bin\",\n",
    "    binary = True\n",
    "    )\n",
    "\n",
    "embedder = Word2Vec(size=300, min_count=1)\n",
    "embedder.build_vocab(token_train_list)\n",
    "total_examples = embedder.corpus_count\n",
    "embedder.build_vocab([list(model.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.intersect_word2vec_format(\"../models/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/5rwzylmj2kz6wnd9vy4t2lhh0000gn/T/ipykernel_28454/1153265867.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  embedder.train(token_train_list, total_examples=total_examples, epochs=embedder.iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(303, 735)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.train(token_train_list, total_examples=total_examples, epochs=embedder.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.wv.save_word2vec_format(\"../models/BGL-fine-tune-embedder.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('anomaly_detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb13736d41bd87c4a6da44de2a53cc3b2c3a6ec16bd51d52b3766a818a83a02f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
