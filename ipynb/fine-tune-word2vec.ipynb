{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/qhf5nsts0_77mx3gxd3731800000gn/T/ipykernel_83118/3606537976.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#######\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"../Drain_result/thunderbird_10M.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"../Drain_result/thunderbird_10M.log_templates.csv\")\n",
    "\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "split_date = struct_log[struct_log.Label == 1].Date.values[0]\n",
    "\n",
    "trainset = struct_log[struct_log.Date < split_date]\n",
    "testset = struct_log[struct_log.Date >= split_date] \n",
    "eventid_train = trainset.EventId.unique() \n",
    "eventid_test = testset.EventId.unique() \n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n",
    "\n",
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/qhf5nsts0_77mx3gxd3731800000gn/T/ipykernel_83118/1848025451.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
      "/var/folders/jy/qhf5nsts0_77mx3gxd3731800000gn/T/ipykernel_83118/1848025451.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainset[\"Token\"] = trainset.EventId.map(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "trainset[\"Token\"] = trainset.EventId.map(\n",
    "    lambda id: template_log_train[template_log_train.EventId == id].\n",
    "    EventTemplateIdent_token.values[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# clean special character\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "test_ratio = 0.4\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "\n",
    "trainset, testset = train_test_split(struct_log, test_size=0.4, random_state=seed, shuffle=False)\n",
    "trainset = trainset[trainset[\"Label\"] == 0]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n",
    "\n",
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z][a-z]+|\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]\n",
    "\n",
    "template_log_train[\"EventTemplateIdent_token\"] = pd.Series(token_train_list)\n",
    "trainset[\"Token\"] = trainset.EventId.map(\n",
    "    lambda id: template_log_train[template_log_train.EventId == id].\n",
    "    EventTemplateIdent_token.values[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_log_train.to_pickle(\"./output/BGL/processed/log_template.trainset.pkl\")\n",
    "trainset.to_pickle(\"./output/BGL/processed/log_structured.trainset.pkl\")\n",
    "testset.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")\n",
    "template_log_test.to_pickle(\"./output/BGL/processed/log_structured.testset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \"GoogleNews-vectors-negative300.bin\" from:\n",
    "# https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"../google_news/GoogleNews-vectors-negative300.bin\",\n",
    "    binary = True\n",
    "    )\n",
    "\n",
    "embedder = Word2Vec(vector_size=300, min_count=1)\n",
    "embedder.build_vocab(token_train_list)\n",
    "total_examples = embedder.corpus_count\n",
    "embedder.build_vocab([list(model.key_to_index.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.wv.vectors_lockf = np.ones(len(embedder.wv), dtype=np.float32)\n",
    "embedder.wv.intersect_word2vec_format(\"../google_news/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695519, 725400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.train(token_train_list, total_examples=total_examples, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.wv.save_word2vec_format(\"../thunderbird_10M-fine-tune-embedder.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce4081ce2b9ab9ecfe855a4e9c840ae25394b5d73782b2027cad0c1ddfa0aa02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
