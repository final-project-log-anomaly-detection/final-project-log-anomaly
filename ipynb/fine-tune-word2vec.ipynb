{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "split_date = struct_log[struct_log.Label == 1].Date.values[0]\n",
    "\n",
    "trainset = struct_log[struct_log.Date < split_date]\n",
    "testset = struct_log[struct_log.Date >= split_date]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n",
    "\n",
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data (Splited by timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def text_cleansing(text):\n",
    "    regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "    regex_expect_words = r'[^\\w<>]+'\n",
    "    output = re.sub(regex_except_token, '', text)\n",
    "    output = re.sub(regex_expect_words, ' ', output)\n",
    "    return output\n",
    "\n",
    "struct_log = pd.read_csv(\"./output/BGL/BGL.log_structured.csv\")\n",
    "template_log = pd.read_csv(\"./output/BGL/BGL.log_templates.csv\")\n",
    "\n",
    "test_ratio = 0.4\n",
    "struct_log[\"Label\"] = struct_log[\"Label\"].apply(lambda x: int(x != \"-\"))\n",
    "struct_log.sort_values(\"Time\", inplace=True)\n",
    "struct_log[struct_log[\"Label\"] == 1].Date.value_counts().sort_index()\n",
    "\n",
    "trainset, testset = train_test_split(struct_log, test_size=0.4, random_state=seed, shuffle=False)\n",
    "trainset = trainset[trainset[\"Label\"] == 0]\n",
    "eventid_train = trainset.EventId.unique()\n",
    "eventid_test = testset.EventId.unique()\n",
    "\n",
    "template_log_train = template_log[template_log[\"EventId\"].isin(eventid_train)]\n",
    "template_log_test = template_log[template_log[\"EventId\"].isin(eventid_test)]\n",
    "template_log_train[\"EventTemplateIdent_cleansed\"] = template_log_train.EventTemplateIdent.map(text_cleansing)\n",
    "\n",
    "template_log_train_list = template_log_train[\"EventTemplateIdent_cleansed\"].astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_train_list = [ tokenizer.tokenize(sen) for sen in template_log_train_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"../models/GoogleNews-vectors-negative300.bin\",\n",
    "    binary = True\n",
    "    )\n",
    "\n",
    "embedder = Word2Vec(size=300, min_count=1)\n",
    "embedder.build_vocab(token_train_list)\n",
    "total_examples = embedder.corpus_count\n",
    "embedder.build_vocab([list(model.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.intersect_word2vec_format(\"../models/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.train(token_train_list, total_examples=total_examples, epochs=embedder.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.save_word2vec_format(\"../models/BGL-fine-tune-embedder.txt\", binary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('anomaly_detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 15:49:01) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb13736d41bd87c4a6da44de2a53cc3b2c3a6ec16bd51d52b3766a818a83a02f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
