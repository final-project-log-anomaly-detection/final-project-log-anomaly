{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:03:37.714412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=False):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((data_size - 1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "            \n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextCNN():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        session,\n",
    "        num_classes,\n",
    "        vocab_size,\n",
    "        embedding_size, \n",
    "        filter_sizes, \n",
    "        num_filters,\n",
    "        sequence_length=None,\n",
    "        l2_reg_lambda=0.0,\n",
    "        seed=42,\n",
    "        ) -> None:\n",
    "        \n",
    "        self.sess = session\n",
    "        if seed is not None: tf.random.set_seed(seed)\n",
    "        self.input_words = tf.compat.v1.placeholder(tf.dtypes.int32, shape=(None, sequence_length), name=\"input_words_idx\")\n",
    "        self.input_labels = tf.compat.v1.placeholder(tf.dtypes.float32, shape=(None, num_classes), name=\"input_label\")\n",
    "        self.dropout_keep_prob = tf.compat.v1.placeholder(tf.dtypes.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        l2_loss = tf.constant(0.0, dtype=tf.dtypes.float32)\n",
    "        \n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"): \n",
    "            self.embedding_dictionary = tf.Variable(\n",
    "                tf.random.uniform(shape=(vocab_size, embedding_size), minval=-1., maxval=1.),\n",
    "                trainable=True,\n",
    "                name=\"embedding_dictionary\"\n",
    "            )\n",
    "            self.embedded_words = tf.nn.embedding_lookup(self.embedding_dictionary, self.input_words)\n",
    "            self.embedded_words_expanded = tf.expand_dims(self.embedded_words, -1)\n",
    "            \n",
    "        pooled_output = list()\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(f\"conv-maxpool-{filter_size}\"):\n",
    "                filter_shape = (filter_size, embedding_size, 1, num_filters)\n",
    "                kernels = tf.Variable(\n",
    "                    tf.random.truncated_normal(filter_shape, stddev=0.1),\n",
    "                    name=f\"kernels-{filter_size}\"\n",
    "                )\n",
    "                bias = tf.Variable(\n",
    "                    tf.constant(0.1, shape=(num_filters,)),\n",
    "                    name=f\"bias-kernels-{filter_size}\"\n",
    "                )\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_words_expanded,\n",
    "                    filters=kernels,\n",
    "                    strides=(1, 1, 1, 1),\n",
    "                    padding=\"VALID\",\n",
    "                    name=f\"conv-{filter_size}\"\n",
    "                )\n",
    "                \n",
    "                feature_maps = tf.nn.relu(\n",
    "                    tf.nn.bias_add(conv, bias),\n",
    "                    name=f\"relu-conv-{filter_size}\"\n",
    "                )\n",
    "                \n",
    "                if sequence_length is not None:\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        feature_maps,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool\"\n",
    "                    )\n",
    "                else:\n",
    "                    pooled = tf.math.reduce_max(\n",
    "                        feature_maps,\n",
    "                        axis=1,\n",
    "                        keepdims=True,\n",
    "                        name=f\"global-max-pooling-conv-{filter_size}\"\n",
    "                    )\n",
    "                \n",
    "                pooled_output.append(pooled)\n",
    "        \n",
    "        total_num_filters = num_filters * len(filter_sizes)\n",
    "        self.pooled_feature = tf.concat(pooled_output, axis=3)\n",
    "        self.pooled_feature_flat = tf.reshape(self.pooled_feature, shape=(-1, total_num_filters))\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.feature_drop = tf.nn.dropout(self.pooled_feature_flat, self.dropout_keep_prob)\n",
    "            \n",
    "        with tf.name_scope(\"output\"):\n",
    "            weight = tf.compat.v1.get_variable(\n",
    "                \"fc-weight\",\n",
    "                shape=(total_num_filters, num_classes),\n",
    "                initializer=tf.initializers.glorot_uniform()\n",
    "            )\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=(num_classes,), name=\"fc-bias\"))\n",
    "            l2_loss += tf.nn.l2_loss(weight)\n",
    "            l2_loss += tf.nn.l2_loss(bias)\n",
    "            self.fc_output = tf.nn.bias_add(tf.matmul(self.feature_drop, weight, name=\"fc-weight\"), bias=bias, name=\"fc-bias\")\n",
    "            self.predictions = tf.argmax(self.fc_output, axis=1, name=\"predictions\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.fc_output, labels=self.input_labels)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_labels, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            \n",
    "    def fit(self, X, y,\n",
    "            num_epochs=100, \n",
    "            batch_size=1, \n",
    "            checkpoint_every=None, \n",
    "            # keep_summaries=True,\n",
    "            eval_X = None,\n",
    "            eval_y = None,\n",
    "            ):\n",
    "\n",
    "            assert not ((eval_X is None) ^ (eval_y is None)),\\\n",
    "                \"evaluation set must be either None or not None \"\n",
    "                \n",
    "            if eval_X is None: eval_X = X\n",
    "            if eval_y is None: eval_y = y\n",
    "            \n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            # self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            self.train_ops = self.optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "            \n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(f\"Writing to {out_dir}\")\n",
    "            \n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            self.train_summary_writer = tf.summary.create_file_writer(train_summary_dir)\n",
    "            \n",
    "            with self.train_summary_writer.as_default():\n",
    "                \n",
    "                # if keep_summaries:\n",
    "                grad_summaries = []\n",
    "                for g, v in grads_and_vars:\n",
    "                    if g is not None:\n",
    "                        grad_hist_summary = tf.compat.v1.summary.histogram(f\"{v.name}/grad/hist\", g)\n",
    "                        sparsity_summary = tf.compat.v1.summary.scalar(f\"{v.name}/grad/sparsity\", tf.nn.zero_fraction(g))\n",
    "                        grad_summaries.append(grad_hist_summary)\n",
    "                        grad_summaries.append(sparsity_summary)\n",
    "                grad_summaries_merged = tf.compat.v1.summary.merge(grad_summaries)\n",
    "                \n",
    "                loss_summary = tf.compat.v1.summary.scalar(\"loss\", self.loss)\n",
    "                acc_summary = tf.compat.v1.summary.scalar(\"accuracy\", self.accuracy)\n",
    "                self.train_summary_op = tf.compat.v1.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "                    \n",
    "                checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "                checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir)\n",
    "                saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n",
    "\n",
    "                self.sess.run(tf.compat.v1.initialize_all_variables())\n",
    "                \n",
    "                batches = batch_iter(list(zip(X, y)), batch_size=batch_size, num_epochs=num_epochs)\n",
    "                iter_num = (int((len(X)-1)/batch_size) + 1) * num_epochs\n",
    "                if checkpoint_every is None: checkpoint_every = iter_num - 1\n",
    "                \n",
    "                batch_summary = list()\n",
    "                for batch in tqdm(batches, total=iter_num, desc=f\"traing (batch_size={batch_size}, max_epochs={num_epochs}) :\"):\n",
    "                    x_batch, y_batch = zip(*batch)\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        self.input_words: x_batch,\n",
    "                        self.input_labels: y_batch,\n",
    "                        self.dropout_keep_prob: 0.5\n",
    "                    }\n",
    "                    _, summary, current_step = self.sess.run([\n",
    "                                                    self.train_ops,\n",
    "                                                    self.train_summary_op,\n",
    "                                                    self.global_step,\n",
    "                                                    ], feed_dict=feed_dict)\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    batch_summary.append(summary)\n",
    "                    if (current_step + 1) % checkpoint_every == 0:\n",
    "                        path = saver.save(self.sess, checkpoint_prefix, global_step=current_step)\n",
    "                        print(f\"Saved model checkpoint to {path}\")\n",
    "                        \n",
    "                        preds, loss, acc = self.predict(eval_X, eval_y)\n",
    "                        print(f\"{time_str}:{current_step}:: evaluation result: loss = {loss}, acc = {acc}\")\n",
    "                        \n",
    "                return batch_summary\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        feed_dict = {\n",
    "                    self.input_words: X,\n",
    "                    self.dropout_keep_prob: 1\n",
    "                }\n",
    "        pipeline = [self.predictions]\n",
    "        \n",
    "        if y is not None:\n",
    "            feed_dict[self.input_labels] = y\n",
    "            pipeline.extend([self.loss, self.accuracy])\n",
    "\n",
    "            preds, loss, acc = self.sess.run(pipeline, feed_dict=feed_dict)\n",
    "            return preds, loss, acc\n",
    "        else:\n",
    "            preds = self.sess.run(pipeline, feed_dict=feed_dict)    \n",
    "            return preds, None, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class DataPreprocessor(object):\n",
    "    \n",
    "    def __init__(self, record_unknow=False) -> None:\n",
    "        if record_unknow: self.unknow_words = dict()\n",
    "    \n",
    "    def load_word2vec_format(self, \n",
    "                             file_path, \n",
    "                             unknow_token=None, \n",
    "                             unknow_repr=None):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            vec = dict()\n",
    "            for l in f.readlines():\n",
    "                data = l.split()\n",
    "                vec[data[0]] = np.array(data[1:], dtype=np.float32)\n",
    "            \n",
    "            f.close()\n",
    "                \n",
    "        embedding_size = vec.pop(list(vec.keys())[0])\n",
    "        if unknow_token is not None:\n",
    "            try:\n",
    "                unknow_vec = vec[unknow_token]\n",
    "            except AttributeError:\n",
    "                print(f\"there's not '{unknow_token}' in dictionary.\")\n",
    "                if unknow_repr:\n",
    "                    assert len(unknow_repr) == embedding_size, \\\n",
    "                        f\"unknown represent vector must same shape with embedding size (expected {embedding_size}, got {len(unknow_repr)})\"\n",
    "                else:\n",
    "                    unknow_repr = [ 0 for _ in range(embedding_size) ]\n",
    "                vec[unknow_token] = unknow_repr\n",
    "            self.unknow_token = unknow_token\n",
    "        else: self.unknow_token = None\n",
    "        \n",
    "        vocab_size = len(vec.keys())    \n",
    "        self.word_vectors = np.array(list(vec.values()))\n",
    "        self.words_indices = { word: i for i, word in enumerate(vec.keys()) }\n",
    "        self.embedding_shape = NamedTuple(vocab_size=vocab_size, embedding_size=embedding_size)\n",
    "    \n",
    "    def indice_encode(self, line):\n",
    "        record_unknow = hasattr(self, \"unknow_words\")\n",
    "        words = line.split()\n",
    "        encoded = list()\n",
    "        for word in words:\n",
    "            try:\n",
    "                i = self.words_indices[word]\n",
    "            except AttributeError:\n",
    "                if record_unknow:\n",
    "                    if word in self.unknow_words.keys(): self.unknow_words[word] += 1\n",
    "                    else: self.unknow_words[word] = 0\n",
    "                \n",
    "                if self.unknow_token is None: continue\n",
    "                else: i = self.words_indices[self.unknow_token]\n",
    "            encoded.append(i)\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def indice_padding(self, batch, padding_token, padding_size=0):\n",
    "        padding_idx = self.words_indices[padding_token]\n",
    "        seq_length = np.array([ len(inst) for inst in batch ])\n",
    "        max_length = seq_length.max()\n",
    "        padding_size = max(padding_size, max_length)\n",
    "        \n",
    "        padding_batch = np.array([ np.pad(inst, (0, padding_size - len(inst)), constant_values=padding_idx) for inst in batch ])\n",
    "        return padding_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10000\n",
    "rand_array = [ \n",
    "            [np.random.randint(0, 20) for _ in range(20)] \\\n",
    "                for _ in range(input_size)\n",
    "            ]\n",
    "# rand_array = [[11, 6, 5, 2, 4, 12], [4, 6, 6, 16, 0, 9], [5, 19, 10, 5, 1, 14]]\n",
    "rand_label = np.array([ [1., 0.] \\\n",
    "    # if np.random.randint(0, 2) == 0 else [0., 1.] \\\n",
    "        for _ in range(input_size) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_size = int(input_size * 0.2)\n",
    "eval_arr = [ \n",
    "            [np.random.randint(0, 20) for _ in range(20)] \\\n",
    "                for _ in range(eval_size)\n",
    "            ]\n",
    "# rand_array = [[11, 6, 5, 2, 4, 12], [4, 6, 6, 16, 0, 9], [5, 19, 10, 5, 1, 14]]\n",
    "eval_label = np.array([ [1., 0.] \\\n",
    "    if np.random.randint(0, 2) == 0 else [0., 1.] \\\n",
    "        for _ in range(eval_size) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/62-409/Documents/Log_Detection/final-project-log-anomaly/ipynb/runs/1673002518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "traing (batch_size=10, max_epochs=5) ::  22%|██▏       | 1076/5000 [00:02<00:10, 390.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/62-409/Documents/Log_Detection/final-project-log-anomaly/ipynb/runs/1673002518/checkpoints/model-999\n",
      "2023-01-06T17:55:21.543900:999:: evaluation result: loss = 0.8319565653800964, acc = 0.49950000643730164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "traing (batch_size=10, max_epochs=5) ::  42%|████▏     | 2083/5000 [00:04<00:06, 456.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/62-409/Documents/Log_Detection/final-project-log-anomaly/ipynb/runs/1673002518/checkpoints/model-1999\n",
      "2023-01-06T17:55:23.646555:1999:: evaluation result: loss = 1.0630583763122559, acc = 0.49950000643730164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "traing (batch_size=10, max_epochs=5) ::  61%|██████    | 3055/5000 [00:06<00:04, 395.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/62-409/Documents/Log_Detection/final-project-log-anomaly/ipynb/runs/1673002518/checkpoints/model-2999\n",
      "2023-01-06T17:55:25.624709:2999:: evaluation result: loss = 1.3203043937683105, acc = 0.49950000643730164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "traing (batch_size=10, max_epochs=5) ::  81%|████████  | 4056/5000 [00:08<00:02, 364.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/62-409/Documents/Log_Detection/final-project-log-anomaly/ipynb/runs/1673002518/checkpoints/model-3999\n",
      "2023-01-06T17:55:27.741025:3999:: evaluation result: loss = 1.582446575164795, acc = 0.49950000643730164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "traing (batch_size=10, max_epochs=5) :: 100%|██████████| 5000/5000 [00:11<00:00, 452.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /Users/62-409/Documents/Log_Detection/final-project-log-anomaly/ipynb/runs/1673002518/checkpoints/model-4999\n",
      "2023-01-06T17:55:30.015259:4999:: evaluation result: loss = 1.8297275304794312, acc = 0.49950000643730164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    allow_soft_placement=True\n",
    "    log_device_placement=False\n",
    "    \n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "    allow_soft_placement=allow_soft_placement,\n",
    "    log_device_placement=log_device_placement)\n",
    "    sess = tf.compat.v1.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        \n",
    "        model = TextCNN(\n",
    "            session=sess,\n",
    "            num_classes=2,\n",
    "            vocab_size=20,\n",
    "            embedding_size=6,\n",
    "            filter_sizes=[2, 3],\n",
    "            num_filters=2,\n",
    "        )\n",
    "        \n",
    "        sum = model.fit(\n",
    "            X=rand_array, \n",
    "            y=rand_label,\n",
    "            batch_size=10,\n",
    "            num_epochs=5,\n",
    "            checkpoint_every=1000,\n",
    "            # keep_summaries=True,\n",
    "            eval_X=eval_arr,\n",
    "            eval_y=eval_label,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b26b62bca4b7f0e106be39e9c6e89766e9ee255e4f89d8c593ae0eb8bab8bc16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
