{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22975,"status":"ok","timestamp":1674550290844,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"hkcNmXTvnIcQ","outputId":"97b56c14-e301-4507-89ae-63727b52e1bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4022,"status":"ok","timestamp":1674550294863,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"H6HmoMNrm_iW"},"outputs":[],"source":["import tensorflow as tf # tensorflow v2.11.0\n","import numpy as np"]},{"cell_type":"code","source":["import random\n","import string\n","\n","# printing lowercase\n","letters = string.ascii_lowercase\n","\n","with open(\"./text.txt\", \"w\") as file:\n","  for i in range(100):\n","    text = \"\".join([ random.choice(letters) for _ in range(random.randint(1, 10)) ])\n","    line = \" \".join(list(np.random.permutation(300).astype(str)))\n","    file.writelines(\"{} {}\\n\".format(text, line))\n","\n","  file.close()"],"metadata":{"id":"yf4Iutbh-YkT","executionInfo":{"status":"ok","timestamp":1674550880552,"user_tz":-420,"elapsed":2,"user":{"displayName":"No Noseason","userId":"05219773639042295484"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["file = open(\"./text.txt\", \"r\")"],"metadata":{"id":"gI3FvmWbAwyX","executionInfo":{"status":"ok","timestamp":1674551226074,"user_tz":-420,"elapsed":3,"user":{"displayName":"No Noseason","userId":"05219773639042295484"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":5724,"status":"ok","timestamp":1674551292198,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"uNHFrdYdsnS1"},"outputs":[],"source":["import sys\n","sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/final_project/final-project-log-anomaly\")\n","\n","from common.debug_utils import *\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYS9f3cRxrm6"},"outputs":[],"source":["import psutil\n","\n","def test_ram(length):\n","  vec = dict()\n","  availb_mem = psutil.virtual_memory().available\n","  print(\"Available RAM: {}\".format(as_Bytes(availb_mem)))\n","  for i in (pbar := tqdm(range(length))) :\n","    vec[\"ID\" + f\"0{i}\"[:-2]] = np.array([i/length, length/(i+1), i*i, length*length])\n","    availb_mem = psutil.virtual_memory().available\n","    desc = \"RAM={} (Available RAM={}):progress::\".format(\n","        as_Bytes(Bytes.getsize(vec)), as_Bytes(availb_mem)\n","        )\n","    pbar.set_description(desc)\n","\n","  del vec\n","  availb_mem = psutil.virtual_memory().available\n","  print(\"Available RAM: {}\".format(as_Bytes(availb_mem)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtGiGLHhm_is"},"outputs":[],"source":["import re\n","from sklearn.preprocessing import OneHotEncoder\n","from typing import NamedTuple\n","\n","EmbeddingShape = NamedTuple(\"EmbeddingShape\", [(\"vocab_size\", int), (\"embedding_size\", int)])\n","'''(NamedTuple) class for embedding shape contain vocab_size and embedding_size'''\n","\n","class LogPreprocessor(object):\n","    \"\"\"LogPreprocessor\n","    class for preprocessing data before feed to model\n","    \n","    Args:\n","        record_unknow (bool): whether recording unknown word from preprocessing process or not. Defaults to False.\n","    \"\"\"\n","    \n","    def __init__(self, record_unknow=False) -> None:\n","        if record_unknow: \n","            self.unknow_words = dict()\n","            '''dictionary for counting ocurred unknow word from preprocessing process where keys is word and value is occuring number'''\n","    \n","    @staticmethod\n","    def text_cleansing(text):\n","        \"\"\"(static method) text_cleansing.\n","        method for cleansing log text.\n","        \n","        Args:\n","            text (str): log text to cleansing special character.\n","\n","        Returns:\n","            str: cleansed log text.\n","        \"\"\"\n","        regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n","        regex_expect_words = r'[^\\w<>]+'\n","        output = re.sub(regex_except_token, '', text)\n","        output = re.sub(regex_expect_words, ' ', output)\n","        return output\n","    \n","    def load_word2vec_format(self, \n","                             file_path, \n","                             unknow_token=None, \n","                             unknow_repr=None):\n","        \"\"\"(instance method) load_word2vec_format\n","        loading word2vec format file to extract embeadding metrix\n","\n","        Args:\n","            file_path (str): file's path to word2vec format file (.txt)\n","            unknow_token (str, optional): word for represent unknown word (e.g. '<OOV>'). \n","            Defaults to None.\n","            unknow_repr (_ArrayLike, optional): word vector for represent unknown word. \n","            if 'unknow_repr' is not set but 'unknow_token' is set \n","            it will use zero vector as unknow represent vector. Defaults to None.\n","        \"\"\"\n","        embedding_size = None\n","        vocab_size = None\n","        with open(file_path, \"r\") as f:\n","            vec = dict()\n","            GB = 1024*1024*1024\n","            print(\"start load embedding matrix from: {}\".format(file_path))\n","            for l in (pbar := tqdm(f)):\n","                availb_mem = psutil.virtual_memory().available\n","                pbar.set_description(\"Available RAM: {}\".format(as_Bytes(availb_mem)))\n","\n","                data = list(filter(None, re.split(\" +\", l)))\n","                vec[data[0]] = np.array(data[1:], dtype=np.float32)\n","                # if embedding_size is None: embedding_size = int(data[-1])\n","                # elif len(vec[data[0]]) != embedding_size:\n","                #   print(repr(data[0]), vec[data[0]].shape)\n","                #   print(\"line :\", l)\n","\n","                # if availb_mem <= 5*GB: \n","                #   print(\"variable size is exceed 5GB, ({})\".format(as_Bytes(availb_mem)))\n","                #   break\n","                # else: print(as_Bytes(availb_mem))\n","            f.close()\n","                \n","        embedding_size = int(vec.pop(list(vec.keys())[0])[0])\n","        if unknow_token is not None:\n","            try:\n","                unknow_vec = vec[unknow_token]\n","            except AttributeError:\n","                print(f\"there's not '{unknow_token}' in dictionary.\")\n","                if unknow_repr:\n","                    assert len(unknow_repr) == embedding_size, \\\n","                        f\"unknown represent vector must same shape with embedding size (expected {embedding_size}, got {len(unknow_repr)})\"\n","                else:\n","                    unknow_repr = [ 0 for _ in range(embedding_size) ]\n","                vec[unknow_token] = unknow_repr\n","            self.unknow_token = unknow_token\n","        else: \n","            self.unknow_token = None\n","        \n","        vocab_size = len(vec.keys())    \n","        self.word_vectors = np.array(list(vec.values()))\n","        self.words_indices = { word: i for i, word in enumerate(vec.keys()) }\n","        self.embedding_shape = EmbeddingShape(vocab_size, embedding_size)\n","    \n","    def indice_encode(self, line):\n","        \"\"\"(instance method) indice_encode.\n","        encoding string words in log line to index number.\n","\n","        Args:\n","            line (_ArrayLike[str]): array of tokenized string line.\n","\n","        Returns:\n","            List[int]: list contains index number.\n","        \"\"\"\n","        record_unknow = hasattr(self, \"unknow_words\")\n","        encoded = list()\n","        for word in line:\n","            try:\n","                i = self.words_indices[word]\n","            except KeyError:\n","                if record_unknow:\n","                    if word in self.unknow_words.keys(): self.unknow_words[word] += 1\n","                    else: self.unknow_words[word] = 0\n","                \n","                if self.unknow_token is None: continue\n","                else: i = self.words_indices[self.unknow_token]\n","            encoded.append(i)\n","            \n","        return encoded\n","    \n","    def insert_new_word(self, word, vector, index=-1):\n","        \"\"\"(instance method) insert_new_word\n","        insert a new word and vector to word dictionary at specified index\n","        \n","        Args:\n","            word (str): new word to insert\n","            vector (_ArrayLike[Float]): new vector to insert\n","            index (int, optional): insert at index if negative index will insert at (vocab_size + index + 1). Defaults to -1.\n","        \"\"\"\n","        if index < 0: index = self.embedding_shape.vocab_size + index + 1\n","        assert word not in self.words_indices.keys(), \\\n","            f\"there is already exist input word in vocab at {self.words_indices[word]} index\"\n","        assert np.ndim(vector) == 1, \\\n","            f\"expect 1 dim vector as a input but got {np.ndim(vector)}.\"\n","        assert len(vector) == self.embedding_shape.embedding_size, \\\n","            f\"insert vector's shape must match to embedding size. (got {np.shape(vector)})\"\n","            \n","        new_word_vectors = np.insert(self.word_vectors, index, vector, axis=0)\n","        new_words_indices = { k: v if v < index else v + 1 for k, v in self.words_indices.items() }\n","        new_words_indices[word] = index\n","        new_embedding_shape = EmbeddingShape(self.embedding_shape.vocab_size + 1, self.embedding_shape.embedding_size)\n","        \n","        self.word_vectors = new_word_vectors\n","        self.words_indices = new_words_indices\n","        self.embedding_shape = new_embedding_shape\n","    \n","    def indice_padding(self, batch, padding_token, padding_size=0):\n","        \"\"\"(instance method) indice_padding.\n","        padding tokenized array in batch by index number of padding_token.\n","        \n","        Args:\n","            batch (_ArrayLike[_ArrayLike[int]]): data batch of array of index number.\n","            padding_token (str): word for represent padding token (e.g. \"<PADDING>\") where padding token must in dictionary.\n","            padding_size (int, optional): size of output. if 'padding_size' is less than longest line in batch it will set to length of longest line is batch. Defaults to 0.\n","\n","        Returns:\n","            NDArray[NDArray[int]]: array of padded batch data.\n","        \"\"\"\n","        padding_idx = self.words_indices[padding_token]\n","        seq_length = np.array([ len(inst) for inst in batch ])\n","        max_length = seq_length.max()\n","        padding_size = max(padding_size, max_length)\n","        \n","        padding_batch = np.array([ np.pad(inst, (0, padding_size - len(inst)), constant_values=padding_idx) for inst in batch ])\n","        return padding_batch\n","    \n","    @staticmethod\n","    def build_label(y, classes=None):\n","        \"\"\"(static method) build_label\n","        build labels array using one-hot encoding\n","\n","        Args:\n","            y (_ArrayLike): input labels\n","            classes (_ArrayLike, optional): array contains all classes. if None, it will set to unique label of input. Defaults to None.\n","\n","        Returns:\n","            _Array[_Array[int]]: array of one-hot label where shape is (input_size, num_classes)\n","        \"\"\"\n","        if classes is None:\n","            classes = np.unique(y)\n","            \n","        encoder = OneHotEncoder(categories=[classes])\n","        return encoder.fit_transform(y).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBACP6l4n0Rt"},"outputs":[],"source":["import os\n","from zipfile import ZipFile\n","\n","word2vec_path = \"/content/drive/MyDrive/Colab Notebooks/word2Vec\"\n","word2vec_zip = word2vec_path + \"/AIT_apache2_word2Vec-fine-tune-embedder.txt.zip\"\n","word2vec_files = word2vec_path + \"/AIT_apache2_word2Vec-fine-tune-embedder.txt\"\n","if not os.path.exists(word2vec_files):\n","  with ZipFile(word2vec_zip, 'r') as zf:\n","    zf.extractall(path=word2vec_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6X1x1l31QGfp","outputId":"0ed2930d-8535-41c3-d84f-97ebc934d87c"},"outputs":[{"name":"stdout","output_type":"stream","text":["start load embedding matrix from: /content/drive/MyDrive/Colab Notebooks/word2Vec/AIT_apache2_word2Vec-fine-tune-embedder.txt\n"]},{"name":"stderr","output_type":"stream","text":["Available RAM: 7.70 GB: : 2289117it [1:59:02, 320.49it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-c8d45bb418d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# extract word2vec format file to embedding matrix and words indice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# change this path for select another embedding method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-03fb0bc815c2>\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(self, file_path, unknow_token, unknow_repr)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mavailb_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available RAM: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_Bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailb_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" +\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_description\u001b[0;34m(self, desc, refresh)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_description_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["prep = LogPreprocessor()\n","# extract word2vec format file to embedding matrix and words indice\n","# change this path for select another embedding method\n","prep.load_word2vec_format(word2vec_files)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9wTg873m_iw"},"outputs":[],"source":["# insert \"<PAD>\" at first index of embedding matrix with zeros vector\n","# if padding token is represented as zeros vector, unknown token must be represented\n","# as non-zeros vector. (that mean if you set unknow_token for load_word2vec_format you\n","# have to set unknow_repr too or using non-zero vector for represent padding token).\n","prep.insert_new_word(\"<PAD>\", np.zeros(prep.embedding_shape.embedding_size), index=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWyJp9Dnm_ix"},"outputs":[],"source":["import pandas as pd # pandas v1.5.2\n","\n","prep_dataset_path = \"/content/drive/MyDrive/Colab Notebooks/AIT_preprocessed_type2_apache2_mail.cup.com\"\n","# load processed trainset of structured log and template log data\n","prep_train_struc_log = pd.read_pickle(prep_dataset_path + \"/train_set.pkl\")\n","prep_train_templ_log = pd.read_pickle(prep_dataset_path + \"/template_train_set.pkl\")\n","prep_train_struc_log[\"Label\"] = prep_train_struc_log[[\"time_label\", \"line_label\"]]\\\n","    .apply(lambda x: x[0] or x[1], axis=1).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7CHNyphm_iy"},"outputs":[],"source":["# encode token to index of embedding matrix\n","prep_train_struc_log[\"Token_Indice_encoded\"] = prep_train_struc_log.Token.map(prep.indice_encode)\n","# padding token with \"<PAD>\"\n","padd_input = prep.indice_padding(prep_train_struc_log[\"Token_Indice_encoded\"], \"<PAD>\")\n","# build label array to one hot format\n","label = prep.build_label(prep_train_struc_log[\"Label\"].to_numpy().reshape((-1, 1)), classes=[0, 1])\n","# you also can use this preprocessing pipeline for testset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GK8HKZ2Km_iy"},"outputs":[],"source":["# testset pipeline\n","prep_test_struc_log = pd.read_pickle(prep_dataset_path + \"/test_set.pkl\")\n","prep_test_templ_log = pd.read_pickle(prep_dataset_path + \"/template_test_set.pkl\")\n","\n","convert_abnormal_labels = lambda x: 0 if x == '0' else 1\n","prep_test_struc_log[\"time_label\"] = prep_test_struc_log[\"time_label\"].map(convert_abnormal_labels)\n","prep_test_struc_log[\"line_label\"] = prep_test_struc_log[\"line_label\"].map(convert_abnormal_labels)\n","\n","prep_test_struc_log[\"Label\"] = prep_test_struc_log[[\"time_label\", \"line_label\"]]\\\n","    .apply(lambda x: x[0] or x[1], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UycQ33cYm_iz"},"outputs":[],"source":["prep_test_struc_log[\"Token_Indice_encoded\"] = prep_test_struc_log.Token.map(prep.indice_encode)\n","\n","padd_test_input = prep.indice_padding(prep_test_struc_log[\"Token_Indice_encoded\"], \"<PAD>\")\n","\n","test_label = prep.build_label(prep_test_struc_log[\"Label\"].to_numpy().reshape((-1, 1)), classes=[0, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zi31rnhlm_i0"},"outputs":[],"source":["class TextCNNConfig():\n","    \"\"\"class for containing config of TextCNN model\n","    \"\"\"\n","    \n","    def __init__(self,\n","                num_classes,\n","                vocab_size,\n","                embedding_size, \n","                filter_sizes, \n","                num_filters,\n","                sequence_length=None,\n","                dropout_rate=None,\n","                l2_reg_lambda=0.0,\n","                seed=42,\n","                pretrain_embedding_matrix=None\n","                ) -> None:\n","        self.num_classes = num_classes\n","        self.vocab_size = vocab_size\n","        self.embedding_size = embedding_size\n","        self.filter_sizes = filter_sizes\n","        self.num_filters = num_filters\n","        self.sequence_length = sequence_length\n","        self.dropout_rate = dropout_rate\n","        self.l2_reg_lambda = l2_reg_lambda\n","        self.seed = seed\n","        self.pretrain_embedding_matrix = pretrain_embedding_matrix\n","       \n","    def __repr__(self):\n","        return \"TextCNNConfig:\\n\" +\\\n","            f\"num_classes:      {self.num_classes}\\n\" +\\\n","            f\"vocab_sizes:      {self.vocab_size}\\n\" +\\\n","            f\"embedding_size:   {self.embedding_size}\\n\" +\\\n","            f\"filter_sizes:     {self.filter_sizes}\\n\" +\\\n","            f\"num_filters:      {self.num_filters}\\n\" +\\\n","            f\"sequence_length:  {self.sequence_length}\\n\" +\\\n","            f\"dropout_rate:     {self.dropout_rate}\\n\" +\\\n","            f\"l2_reg_lambda:    {self.l2_reg_lambda}\\n\" +\\\n","            f\"seed:             {self.seed}\\n\" +\\\n","            f\"pretrain_embedding_matrix's shape: {np.shape(self.pretrain_embedding_matrix) if self.pretrain_embedding_matrix is not None else 'None'}\"\n","\n","class TextCNN(object):\n","    \"\"\"CNN model for NLP task presented by Kim Y. in 2014 \" Convolutional Neural Networks for Sentence Classification\"\n","\n","    Args:\n","        num_classes (int): number of classes for classification.\n","        vocab_size (int): number of known vocabularies. if pretrain_embedding_matrix is set, vocab_size must rely on pretrain_embedding_matrix' shape.\n","        embedding_size (int): length of embedding vector. if pretrain_embedding_matrix is set, embedding_size must rely on pretrain_embedding_matrix' shape.\n","        filter_sizes (_ArrayLike[int]): array contain convolution filter height.\n","        num_filters (int): number of each filter size.\n","        sequence_length (int, optional): length of input sequence. if None, input can be any length. Defualts is None.\n","        dropout_rate (float, optionanl): dropout rate for dropout layer. if None, model will not include dropout layer. Defualts is None.\n","        l2_reg_lambda (float, optional): L2 regularization factor for dense layer. Defaults is 0.0\n","        seed (int, optional): random seed. Defaults is 42.\n","        pretrain_embedding_matrix (_2DArray[float]): pretained embedding vectors. if None, embedding matrix will be initialized by random and it will be trained while trainning else it will be not trainable. Defaults is None\n","    \"\"\"\n","    \n","    def __init__(self,\n","        num_classes,\n","        vocab_size,\n","        embedding_size, \n","        filter_sizes, \n","        num_filters,\n","        sequence_length=None,\n","        dropout_rate=None,\n","        l2_reg_lambda=0.0,\n","        seed=42,\n","        pretrain_embedding_matrix=None\n","        ) -> None:\n","        \n","        self.config = TextCNNConfig(\n","            num_classes,\n","            vocab_size,\n","            embedding_size, \n","            filter_sizes, \n","            num_filters,\n","            sequence_length,\n","            dropout_rate,\n","            l2_reg_lambda,\n","            seed,\n","            pretrain_embedding_matrix\n","        )\n","        \n","        # set global random seed\n","        if seed is not None: tf.random.set_seed(seed)\n","        # define input layer where input shape is (batch_size, sequence_length)\n","        # input data is 2d-array which each value is index of jth word's word vector \n","        # in embedding metrix of ith line.\n","        input_word_idx = tf.keras.layers.Input(\n","            shape=(sequence_length,),\n","            dtype=tf.dtypes.int32,\n","            name=\"input-word-idx-layer\"\n","        )\n","        \n","        # if pretrain_embedding_matrix is not defined, using random uniform for \n","        # initailize embedding matrix\n","        embed_trainable = pretrain_embedding_matrix is None\n","        if pretrain_embedding_matrix is None:\n","            embed_initializers = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n","        else:\n","            pretrain_embedding_matrix = np.array(pretrain_embedding_matrix)\n","            assert (vocab_size, embedding_size) == pretrain_embedding_matrix.shape, \\\n","                f\"shape of embedding_matrix must match to vocab_size and embedding_size (expect {(vocab_size, embedding_size)}, got {pretrain_embedding_matrix.shape}).\"\n","            embed_initializers = tf.keras.initializers.Constant(pretrain_embedding_matrix)\n","        # the embedding layer will defaultly use GPU memory\n","        # so to avoid error while trainning from optimizer which not support GPU\n","        # using tf.device(\"cpu:0\") to place embedding matrix on CPU memory\n","        with tf.name_scope(\"embedding\"), tf.device(\"cpu:0\"):\n","            # define embedding layer where in put is array of index of word's vector in\n","            # embedding matrix. the output's shape is \n","            # (batch_size, sequence_length, embedding_size)\n","            embed = tf.keras.layers.Embedding(\n","                input_dim=vocab_size,\n","                output_dim=embedding_size,\n","                embeddings_initializer=embed_initializers,\n","                input_length=sequence_length,\n","                trainable=embed_trainable,\n","                name=\"embedding-layer\"\n","            )(input_word_idx)\n","            # define reshape layer for expand dimention of output from embedding layer to\n","            # (batch_size, sequence_length, embedding_size, 1) for suit to input of\n","            # 2d convolution layer which require input shape as \n","            # (batch_size, weight, height, channel)\n","            expand_dim_embed = tf.keras.layers.Reshape(\n","                target_shape=(-1, embedding_size, 1),\n","                name=\"reshape-expand-dim-layer\"\n","                )(embed)\n","        \n","        features = list() # list for contain output of each pooling layer\n","        with tf.name_scope(\"convolution\"):\n","            # for loop to define each convolution layer of each size of the filter\n","            for size in filter_sizes:\n","                # define 2d convolution layer of n(num_filters) filters with kernel size\n","                # is (size, embedding_size) strides by (1, 1) and not use padding.\n","                # using ReLu as activation function also use bias. output's shape of this\n","                # layer is (batch_size, sequence_length - size + 1, 1, num_filters)\n","                conv = tf.keras.layers.Conv2D(\n","                    filters=num_filters,\n","                    kernel_size=(size, embedding_size),\n","                    strides=(1, 1),\n","                    padding=\"valid\",\n","                    activation=\"relu\",\n","                    use_bias=True,\n","                    kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1),\n","                    bias_initializer=tf.keras.initializers.Constant(0.1),\n","                    name=f\"conv-{size}_{embedding_size}-layer\"\n","                )(expand_dim_embed)\n","                # define reshape layer for squeeze dimention of output from convolution layer \n","                # to (batch_size, feature_map_height, num_filters)\n","                reshape = tf.keras.layers.Reshape(target_shape=(-1, num_filters), name=f\"reshape-squeeze-{size}_{embedding_size}-layer\")(conv)\n","                # define max-over-time pooling layer for extract max value from each \n","                # feature map from convolution layers. where output is vector with \n","                # shape of (batch_size, 1, num_filters)\n","                pooling = tf.keras.layers.GlobalMaxPool1D(name=f\"global-max-pooling-{size}_{embedding_size}-layer\")(reshape)\n","                # define flatten layer for shape output of pooling layer to \n","                # (batch_size, num_filters)\n","                flatten_pooling = tf.keras.layers.Flatten(name=f\"flatten-pooling-{size}_{embedding_size}-layer\")(pooling)\n","                features.append(flatten_pooling)\n","        \n","        # define concatenate layer to concate all features for convolution & pooling \n","        # process. where output shape is (batch_size, num_filters * len(filter_sizes))\n","        concat = tf.keras.layers.Concatenate(name=\"concatenate-layer\")(features)\n","        \n","        # if dropout_rate is None, model will not include dropout layer\n","        if dropout_rate is not None:\n","            with tf.name_scope(\"dropout\"):\n","                # define dropout layer with specified dropout_rate\n","                dropout = tf.keras.layers.Dropout(rate=dropout_rate)(concat)\n","                fc_input = dropout\n","        else: fc_input = concat\n","        \n","        with tf.name_scope(\"fully-connected\"):\n","            # define output layer (fully connected layer) using Softmax as activation\n","            # and L2 as regularization method. where output is propability to be each \n","            # class with output shape is (batch_size, num_classes).\n","            output = tf.keras.layers.Dense(\n","                units=num_classes,\n","                activation=\"softmax\",\n","                use_bias=True,\n","                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n","                bias_initializer=tf.keras.initializers.Constant(0.1),\n","                kernel_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n","                bias_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n","                name=\"output-layer\"\n","                )(fc_input)\n","        \n","        # define model with all defined sequent layers\n","        self.model = tf.keras.Model(inputs=input_word_idx, outputs=output)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"elapsed":579,"status":"error","timestamp":1674295121533,"user":{"displayName":"No Noseason","userId":"05219773639042295484"},"user_tz":-420},"id":"S1YhxYZ4m_i1","outputId":"9a37fff0-d8b9-4c73-a531-cf5a9eea8d7e"},"outputs":[{"ename":"AssertionError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-43b1cfea1228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m textcnn = TextCNN(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-99c6a4e1cdfa>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, sequence_length, dropout_rate, l2_reg_lambda, seed, pretrain_embedding_matrix)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mpretrain_embedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpretrain_embedding_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;34mf\"shape of embedding_matrix must match to vocab_size and embedding_size (expect {(vocab_size, embedding_size)}, got {pretrain_embedding_matrix.shape}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0membed_initializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_embedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: shape of embedding_matrix must match to vocab_size and embedding_size (expect (15887, 300), got (16186,))."]}],"source":["textcnn = TextCNN(\n","                num_classes=2,\n","                vocab_size=prep.embedding_shape.vocab_size,\n","                embedding_size=prep.embedding_shape.embedding_size,\n","                filter_sizes=[2, 3, 4],\n","                num_filters=2,\n","                sequence_length=None,\n","                dropout_rate=0.5,\n","                l2_reg_lambda=0.01,\n","                seed=42,\n","                pretrain_embedding_matrix=prep.word_vectors)\n","\n","textcnn.config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zWdkbSxm_i2","outputId":"51549cf6-507b-40fc-d60a-d23dbf731eec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input-word-idx-layer (InputLay  [(None, None)]      0           []                               \n"," er)                                                                                              \n","                                                                                                  \n"," embedding-layer (Embedding)    (None, None, 300)    900097200   ['input-word-idx-layer[0][0]']   \n","                                                                                                  \n"," reshape-expand-dim-layer (Resh  (None, None, 300, 1  0          ['embedding-layer[0][0]']        \n"," ape)                           )                                                                 \n","                                                                                                  \n"," conv-2_300-layer (Conv2D)      (None, None, 1, 2)   1202        ['reshape-expand-dim-layer[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," conv-3_300-layer (Conv2D)      (None, None, 1, 2)   1802        ['reshape-expand-dim-layer[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," conv-4_300-layer (Conv2D)      (None, None, 1, 2)   2402        ['reshape-expand-dim-layer[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," reshape-squeeze-2_300-layer (R  (None, None, 2)     0           ['conv-2_300-layer[0][0]']       \n"," eshape)                                                                                          \n","                                                                                                  \n"," reshape-squeeze-3_300-layer (R  (None, None, 2)     0           ['conv-3_300-layer[0][0]']       \n"," eshape)                                                                                          \n","                                                                                                  \n"," reshape-squeeze-4_300-layer (R  (None, None, 2)     0           ['conv-4_300-layer[0][0]']       \n"," eshape)                                                                                          \n","                                                                                                  \n"," global-max-pooling-2_300-layer  (None, 2)           0           ['reshape-squeeze-2_300-layer[0][\n","  (GlobalMaxPooling1D)                                           0]']                             \n","                                                                                                  \n"," global-max-pooling-3_300-layer  (None, 2)           0           ['reshape-squeeze-3_300-layer[0][\n","  (GlobalMaxPooling1D)                                           0]']                             \n","                                                                                                  \n"," global-max-pooling-4_300-layer  (None, 2)           0           ['reshape-squeeze-4_300-layer[0][\n","  (GlobalMaxPooling1D)                                           0]']                             \n","                                                                                                  \n"," flatten-pooling-2_300-layer (F  (None, 2)           0           ['global-max-pooling-2_300-layer[\n"," latten)                                                         0][0]']                          \n","                                                                                                  \n"," flatten-pooling-3_300-layer (F  (None, 2)           0           ['global-max-pooling-3_300-layer[\n"," latten)                                                         0][0]']                          \n","                                                                                                  \n"," flatten-pooling-4_300-layer (F  (None, 2)           0           ['global-max-pooling-4_300-layer[\n"," latten)                                                         0][0]']                          \n","                                                                                                  \n"," concatenate-layer (Concatenate  (None, 6)           0           ['flatten-pooling-2_300-layer[0][\n"," )                                                               0]',                             \n","                                                                  'flatten-pooling-3_300-layer[0][\n","                                                                 0]',                             \n","                                                                  'flatten-pooling-4_300-layer[0][\n","                                                                 0]']                             \n","                                                                                                  \n"," dropout (Dropout)              (None, 6)            0           ['concatenate-layer[0][0]']      \n","                                                                                                  \n"," output-layer (Dense)           (None, 2)            14          ['dropout[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 900,102,620\n","Trainable params: 5,420\n","Non-trainable params: 900,097,200\n","__________________________________________________________________________________________________\n"]}],"source":["# compile model using Adam optimizer with learning rate equal to 0.001\n","# categorical crossentropy as loss function and evaluation metrics are\n","# precision and recall\n","textcnn.model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","    metrics=[\n","        tf.keras.metrics.Precision(),\n","        tf.keras.metrics.Recall()\n","    ]\n",")\n","textcnn.model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE53TxDcm_i4","outputId":"4bb6e43e-2d91-40e0-b9cc-9e84b464f51d"},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}],"source":["# Note: If you don't want to visualize model achitecture \n","# ***** you don't need to run this cell just skip it.\n","# You must install pydot (`pip install pydot`) and install graphviz \n","# (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n","# (e.g. for mac) $ brew install graphviz\n","# (e.g. for win) $ winget install graphviz\n","tf.keras.utils.plot_model(textcnn.model, \"textcnn_model.png\", show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZZYzL-Xm_i4","outputId":"c7854071-76a6-4ede-9625-7832dceac46c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2023-01-15 15:43:06.119443: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n","/opt/miniconda3/lib/python3.9/site-packages/keras/backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["3488/3488 [==============================] - 90s 25ms/step - loss: 0.0394 - precision: 0.9987 - recall: 0.9987\n","Epoch 2/2\n","3488/3488 [==============================] - 93s 27ms/step - loss: 0.0252 - precision: 1.0000 - recall: 1.0000\n"]},{"data":{"text/plain":["{'loss': [0.03941937908530235, 0.02520710974931717],\n"," 'precision': [0.9986650347709656, 1.0],\n"," 'recall': [0.9986650347709656, 1.0]}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["hist = textcnn.model.fit(\n","    x=padd_input,\n","    y=label,\n","    batch_size=None,\n","    epochs=2,\n","    shuffle=False\n",")\n","hist.history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWc0L4lMm_i5"},"outputs":[],"source":["textcnn.model.evaluate(\n","    x=padd_test_input,\n","    y=test_label\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYihsoQMm_i6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyi3uQdtm_i6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7M3tRDim_i7"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"anomaly_detection","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"vscode":{"interpreter":{"hash":"fb13736d41bd87c4a6da44de2a53cc3b2c3a6ec16bd51d52b3766a818a83a02f"}}},"nbformat":4,"nbformat_minor":0}