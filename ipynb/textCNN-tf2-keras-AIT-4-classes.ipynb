{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # tensorflow v2.11.0\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from typing import NamedTuple\n",
    "\n",
    "EmbeddingShape = NamedTuple(\"EmbeddingShape\", [(\"vocab_size\", int), (\"embedding_size\", int)])\n",
    "'''(NamedTuple) class for embedding shape contain vocab_size and embedding_size'''\n",
    "\n",
    "class LogPreprocessor(object):\n",
    "    \"\"\"LogPreprocessor\n",
    "    class for preprocessing data before feed to model\n",
    "    \n",
    "    Args:\n",
    "        record_unknow (bool): whether recording unknown word from preprocessing process or not. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, record_unknow=False) -> None:\n",
    "        if record_unknow: \n",
    "            self.unknow_words = dict()\n",
    "            '''dictionary for counting ocurred unknow word from preprocessing process where keys is word and value is occuring number'''\n",
    "    \n",
    "    @staticmethod\n",
    "    def text_cleansing(text):\n",
    "        \"\"\"(static method) text_cleansing.\n",
    "        method for cleansing log text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): log text to cleansing special character.\n",
    "\n",
    "        Returns:\n",
    "            str: cleansed log text.\n",
    "        \"\"\"\n",
    "        regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "        regex_expect_words = r'[^\\w<>]+'\n",
    "        output = re.sub(regex_except_token, '', text)\n",
    "        output = re.sub(regex_expect_words, ' ', output)\n",
    "        return output\n",
    "    \n",
    "    def load_word2vec_format(self, \n",
    "                             file_path, \n",
    "                             unknow_token=None, \n",
    "                             unknow_repr=None):\n",
    "        \"\"\"(instance method) load_word2vec_format\n",
    "        loading word2vec format file to extract embeadding metrix\n",
    "\n",
    "        Args:\n",
    "            file_path (str): file's path to word2vec format file (.txt)\n",
    "            unknow_token (str, optional): word for represent unknown word (e.g. '<OOV>'). \n",
    "            Defaults to None.\n",
    "            unknow_repr (_ArrayLike, optional): word vector for represent unknown word. \n",
    "            if 'unknow_repr' is not set but 'unknow_token' is set \n",
    "            it will use zero vector as unknow represent vector. Defaults to None.\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            vec = dict()\n",
    "            for l in f.readlines():\n",
    "                data = list(filter(None, re.split(\" +\", l)))\n",
    "                vec[data[0]] = np.array(data[1:], dtype=np.float32)\n",
    "            \n",
    "            f.close()\n",
    "                \n",
    "        embedding_size = int(vec.pop(list(vec.keys())[0])[0])\n",
    "        if unknow_token is not None:\n",
    "            try:\n",
    "                unknow_vec = vec[unknow_token]\n",
    "            except AttributeError:\n",
    "                print(f\"there's not '{unknow_token}' in dictionary.\")\n",
    "                if unknow_repr:\n",
    "                    assert len(unknow_repr) == embedding_size, \\\n",
    "                        f\"unknown represent vector must same shape with embedding size (expected {embedding_size}, got {len(unknow_repr)})\"\n",
    "                else:\n",
    "                    unknow_repr = [ 0 for _ in range(embedding_size) ]\n",
    "                vec[unknow_token] = unknow_repr\n",
    "            self.unknow_token = unknow_token\n",
    "        else: \n",
    "            self.unknow_token = None\n",
    "        \n",
    "        vocab_size = len(vec.keys())    \n",
    "        self.word_vectors = np.array(list(vec.values()))\n",
    "        self.words_indices = { word: i for i, word in enumerate(vec.keys()) }\n",
    "        self.embedding_shape = EmbeddingShape(vocab_size, embedding_size)\n",
    "    \n",
    "    def indice_encode(self, line):\n",
    "        \"\"\"(instance method) indice_encode.\n",
    "        encoding string words in log line to index number.\n",
    "\n",
    "        Args:\n",
    "            line (_ArrayLike[str]): array of tokenized string line.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: list contains index number.\n",
    "        \"\"\"\n",
    "        record_unknow = hasattr(self, \"unknow_words\")\n",
    "        encoded = list()\n",
    "        for word in line:\n",
    "            try:\n",
    "                i = self.words_indices[word]\n",
    "            except KeyError:\n",
    "                if record_unknow:\n",
    "                    if word in self.unknow_words.keys(): self.unknow_words[word] += 1\n",
    "                    else: self.unknow_words[word] = 0\n",
    "                \n",
    "                if self.unknow_token is None: continue\n",
    "                else: i = self.words_indices[self.unknow_token]\n",
    "            encoded.append(i)\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def insert_new_word(self, word, vector, index=-1):\n",
    "        \"\"\"(instance method) insert_new_word\n",
    "        insert a new word and vector to word dictionary at specified index\n",
    "        \n",
    "        Args:\n",
    "            word (str): new word to insert\n",
    "            vector (_ArrayLike[Float]): new vector to insert\n",
    "            index (int, optional): insert at index if negative index will insert at (vocab_size + index + 1). Defaults to -1.\n",
    "        \"\"\"\n",
    "        if index < 0: index = self.embedding_shape.vocab_size + index + 1\n",
    "        assert word not in self.words_indices.keys(), \\\n",
    "            f\"there is already exist input word in vocab at {self.words_indices[word]} index\"\n",
    "        assert np.ndim(vector) == 1, \\\n",
    "            f\"expect 1 dim vector as a input but got {np.ndim(vector)}.\"\n",
    "        assert len(vector) == self.embedding_shape.embedding_size, \\\n",
    "            f\"insert vector's shape must match to embedding size. (got {np.shape(vector)})\"\n",
    "            \n",
    "        new_word_vectors = np.insert(self.word_vectors, index, vector, axis=0)\n",
    "        new_words_indices = { k: v if v < index else v + 1 for k, v in self.words_indices.items() }\n",
    "        new_words_indices[word] = index\n",
    "        new_embedding_shape = EmbeddingShape(self.embedding_shape.vocab_size + 1, self.embedding_shape.embedding_size)\n",
    "        \n",
    "        self.word_vectors = new_word_vectors\n",
    "        self.words_indices = new_words_indices\n",
    "        self.embedding_shape = new_embedding_shape\n",
    "    \n",
    "    def indice_padding(self, batch, padding_token, padding_size=0):\n",
    "        \"\"\"(instance method) indice_padding.\n",
    "        padding tokenized array in batch by index number of padding_token.\n",
    "        \n",
    "        Args:\n",
    "            batch (_ArrayLike[_ArrayLike[int]]): data batch of array of index number.\n",
    "            padding_token (str): word for represent padding token (e.g. \"<PADDING>\") where padding token must in dictionary.\n",
    "            padding_size (int, optional): size of output. if 'padding_size' is less than longest line in batch it will set to length of longest line is batch. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            NDArray[NDArray[int]]: array of padded batch data.\n",
    "        \"\"\"\n",
    "        padding_idx = self.words_indices[padding_token]\n",
    "        seq_length = np.array([ len(inst) for inst in batch ])\n",
    "        max_length = seq_length.max()\n",
    "        padding_size = max(padding_size, max_length)\n",
    "        \n",
    "        padding_batch = np.array([ np.pad(inst, (0, padding_size - len(inst)), constant_values=padding_idx) for inst in batch ])\n",
    "        return padding_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_label(y, classes=None):\n",
    "        \"\"\"(static method) build_label\n",
    "        build labels array using one-hot encoding\n",
    "\n",
    "        Args:\n",
    "            y (_ArrayLike): input labels\n",
    "            classes (_ArrayLike, optional): array contains all classes. if None, it will set to unique label of input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            _Array[_Array[int]]: array of one-hot label where shape is (input_size, num_classes)\n",
    "        \"\"\"\n",
    "        if classes is None:classes = np.unique(y)\n",
    "            \n",
    "        encoder = OneHotEncoder(categories=[classes])\n",
    "        return encoder.fit_transform(y).toarray()\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_mult_label(y, classes=None):\n",
    "        y = np.array(y)\n",
    "        assert np.ndim(y) == 2, \"y must be 2 dimention array.\"\n",
    "\n",
    "        if classes is None:\n",
    "            classes = [ np.unique(feature).tolist() for feature in y.T ]\n",
    "            \n",
    "        encoder = OneHotEncoder(categories=classes)\n",
    "        return encoder.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = LogPreprocessor()\n",
    "# extract word2vec format file to embedding matrix and words indice\n",
    "# change this path for select another embedding method\n",
    "prep.load_word2vec_format(\"../AIT_apache2_word2Vec-fine-tune-embedder.txt\")\n",
    "# insert \"<PAD>\" at first index of embedding matrix with zeros vector\n",
    "# if padding token is represented as zeros vector, unknown token must be represented\n",
    "# as non-zeros vector. (that mean if you set unknow_token for load_word2vec_format you\n",
    "# have to set unknow_repr too or using non-zero vector for represent padding token).\n",
    "prep.insert_new_word(\"<PAD>\", np.zeros(prep.embedding_shape.embedding_size), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load processed trainset of structured log and template log data\n",
    "prep_train_struc_log = pd.read_pickle(\"../data_preprocess/processed_type2/AIT_preprocessed_type2/train_set.pkl\")\n",
    "prep_train_templ_log = pd.read_pickle(\"../data_preprocess/processed_type2/AIT_preprocessed_type2/template_train_set.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode token to index of embedding matrix\n",
    "prep_train_struc_log[\"Token_Indice_encoded\"] = prep_train_struc_log.Token.map(prep.indice_encode)\n",
    "# padding token with \"<PAD>\"\n",
    "padd_input = prep.indice_padding(prep_train_struc_log[\"Token_Indice_encoded\"], \"<PAD>\")\n",
    "# build label array to one hot format\n",
    "y = prep_train_struc_log[[\"time_label\", \"line_label\"]].to_numpy().astype(str)\n",
    "label = LogPreprocessor.build_mult_label(y, classes=[['0', '1'], ['0', '1']])\n",
    "# you also can use this preprocessing pipeline for testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset pipeline\n",
    "prep_test_struc_log = pd.read_pickle(\"../data_preprocess/processed_type2/AIT_preprocessed_type2/test_set.pkl\")\n",
    "prep_test_templ_log = pd.read_pickle(\"../data_preprocess/processed_type2/AIT_preprocessed_type2/template_test_set.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['webshell-netstat-nau', 'hydra', 'webshell-date', 'webshell-cat-passwd', 'mail-curl', 'webshell-netstat-t', 'webshell-id', 'webshell-ps-aux', 'webshell-last', 'upload', 'webshell-w', 'nikto', 'vrfy', 'webshell-netstat-natp', 'webshell-cd-home', 'webshell-netstat-nat', 'webshell-whoami', 'webshell-wget-eximsploitsh'] in column 0 during fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m padd_test_input \u001b[39m=\u001b[39m prep\u001b[39m.\u001b[39mindice_padding(prep_test_struc_log[\u001b[39m\"\u001b[39m\u001b[39mToken_Indice_encoded\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m test_y \u001b[39m=\u001b[39m prep_test_struc_log[[\u001b[39m\"\u001b[39m\u001b[39mtime_label\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mline_label\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m test_label \u001b[39m=\u001b[39m LogPreprocessor\u001b[39m.\u001b[39;49mbuild_mult_label(test_y, classes\u001b[39m=\u001b[39;49m[[\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m], [\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m]])\n",
      "Cell \u001b[0;32mIn[2], line 180\u001b[0m, in \u001b[0;36mLogPreprocessor.build_mult_label\u001b[0;34m(y, classes)\u001b[0m\n\u001b[1;32m    177\u001b[0m     classes \u001b[39m=\u001b[39m [ np\u001b[39m.\u001b[39munique(feature)\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39mT ]\n\u001b[1;32m    179\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(categories\u001b[39m=\u001b[39mclasses)\n\u001b[0;32m--> 180\u001b[0m \u001b[39mreturn\u001b[39;00m encoder\u001b[39m.\u001b[39;49mfit_transform(y)\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/base.py:848\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    847\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:818\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse\n\u001b[1;32m    816\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_infrequent_enabled()\n\u001b[0;32m--> 818\u001b[0m fit_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    819\u001b[0m     X,\n\u001b[1;32m    820\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m    821\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    822\u001b[0m     return_counts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infrequent_enabled,\n\u001b[1;32m    823\u001b[0m )\n\u001b[1;32m    824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infrequent_enabled:\n\u001b[1;32m    825\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_infrequent_category_mapping(\n\u001b[1;32m    826\u001b[0m         fit_results[\u001b[39m\"\u001b[39m\u001b[39mn_samples\u001b[39m\u001b[39m\"\u001b[39m], fit_results[\u001b[39m\"\u001b[39m\u001b[39mcategory_counts\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    827\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:120\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m diff:\n\u001b[1;32m    116\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    117\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound unknown categories \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m in column \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m during fit\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(diff, i)\n\u001b[1;32m    119\u001b[0m         )\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m return_counts:\n\u001b[1;32m    122\u001b[0m     category_counts\u001b[39m.\u001b[39mappend(_get_counts(Xi, cats))\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['webshell-netstat-nau', 'hydra', 'webshell-date', 'webshell-cat-passwd', 'mail-curl', 'webshell-netstat-t', 'webshell-id', 'webshell-ps-aux', 'webshell-last', 'upload', 'webshell-w', 'nikto', 'vrfy', 'webshell-netstat-natp', 'webshell-cd-home', 'webshell-netstat-nat', 'webshell-whoami', 'webshell-wget-eximsploitsh'] in column 0 during fit"
     ]
    }
   ],
   "source": [
    "prep_test_struc_log[\"Token_Indice_encoded\"] = prep_test_struc_log.Token.map(prep.indice_encode)\n",
    "\n",
    "padd_test_input = prep.indice_padding(prep_test_struc_log[\"Token_Indice_encoded\"], \"<PAD>\")\n",
    "\n",
    "test_y = prep_test_struc_log[[\"time_label\", \"line_label\"]].to_numpy().astype(str)\n",
    "test_label = LogPreprocessor.build_mult_label(test_y, classes=[['0', '1'], ['0', '1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNNConfig():\n",
    "    \"\"\"class for containing config of TextCNN model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                num_classes,\n",
    "                vocab_size,\n",
    "                embedding_size, \n",
    "                filter_sizes, \n",
    "                num_filters,\n",
    "                sequence_length=None,\n",
    "                dropout_rate=None,\n",
    "                l2_reg_lambda=0.0,\n",
    "                seed=42,\n",
    "                pretrain_embedding_matrix=None\n",
    "                ) -> None:\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.seed = seed\n",
    "        self.pretrain_embedding_matrix = pretrain_embedding_matrix\n",
    "       \n",
    "    def __repr__(self):\n",
    "        return \"TextCNNConfig:\\n\" +\\\n",
    "            f\"num_classes:      {self.num_classes}\\n\" +\\\n",
    "            f\"vocab_sizes:      {self.vocab_size}\\n\" +\\\n",
    "            f\"embedding_size:   {self.embedding_size}\\n\" +\\\n",
    "            f\"filter_sizes:     {self.filter_sizes}\\n\" +\\\n",
    "            f\"num_filters:      {self.num_filters}\\n\" +\\\n",
    "            f\"sequence_length:  {self.sequence_length}\\n\" +\\\n",
    "            f\"dropout_rate:     {self.dropout_rate}\\n\" +\\\n",
    "            f\"l2_reg_lambda:    {self.l2_reg_lambda}\\n\" +\\\n",
    "            f\"seed:             {self.seed}\\n\" +\\\n",
    "            f\"pretrain_embedding_matrix's shape: {np.shape(self.pretrain_embedding_matrix) if self.pretrain_embedding_matrix is not None else 'None'}\"\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"CNN model for NLP task presented by Kim Y. in 2014 \" Convolutional Neural Networks for Sentence Classification\"\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes for classification.\n",
    "        vocab_size (int): number of known vocabularies. if pretrain_embedding_matrix is set, vocab_size must rely on pretrain_embedding_matrix' shape.\n",
    "        embedding_size (int): length of embedding vector. if pretrain_embedding_matrix is set, embedding_size must rely on pretrain_embedding_matrix' shape.\n",
    "        filter_sizes (_ArrayLike[int]): array contain convolution filter height.\n",
    "        num_filters (int): number of each filter size.\n",
    "        sequence_length (int, optional): length of input sequence. if None, input can be any length. Defualts is None.\n",
    "        dropout_rate (float, optionanl): dropout rate for dropout layer. if None, model will not include dropout layer. Defualts is None.\n",
    "        l2_reg_lambda (float, optional): L2 regularization factor for dense layer. Defaults is 0.0\n",
    "        seed (int, optional): random seed. Defaults is 42.\n",
    "        pretrain_embedding_matrix (_2DArray[float]): pretained embedding vectors. if None, embedding matrix will be initialized by random and it will be trained while trainning else it will be not trainable. Defaults is None\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        num_classes,\n",
    "        vocab_size,\n",
    "        embedding_size, \n",
    "        filter_sizes, \n",
    "        num_filters,\n",
    "        sequence_length=None,\n",
    "        dropout_rate=None,\n",
    "        l2_reg_lambda=0.0,\n",
    "        seed=42,\n",
    "        pretrain_embedding_matrix=None\n",
    "        ) -> None:\n",
    "        \n",
    "        self.config = TextCNNConfig(\n",
    "            num_classes,\n",
    "            vocab_size,\n",
    "            embedding_size, \n",
    "            filter_sizes, \n",
    "            num_filters,\n",
    "            sequence_length,\n",
    "            dropout_rate,\n",
    "            l2_reg_lambda,\n",
    "            seed,\n",
    "            pretrain_embedding_matrix\n",
    "        )\n",
    "        \n",
    "        # set global random seed\n",
    "        if seed is not None: tf.random.set_seed(seed)\n",
    "        # define input layer where input shape is (batch_size, sequence_length)\n",
    "        # input data is 2d-array which each value is index of jth word's word vector \n",
    "        # in embedding metrix of ith line.\n",
    "        input_word_idx = tf.keras.layers.Input(\n",
    "            shape=(sequence_length,),\n",
    "            dtype=tf.dtypes.int32,\n",
    "            name=\"input-word-idx-layer\"\n",
    "        )\n",
    "        \n",
    "        # if pretrain_embedding_matrix is not defined, using random uniform for \n",
    "        # initailize embedding matrix\n",
    "        embed_trainable = pretrain_embedding_matrix is None\n",
    "        if pretrain_embedding_matrix is None:\n",
    "            embed_initializers = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n",
    "        else:\n",
    "            pretrain_embedding_matrix = np.array(pretrain_embedding_matrix)\n",
    "            assert (vocab_size, embedding_size) == pretrain_embedding_matrix.shape, \\\n",
    "                f\"shape of embedding_matrix must match to vocab_size and embedding_size (expect {(vocab_size, embedding_size)}, got {pretrain_embedding_matrix.shape}).\"\n",
    "            embed_initializers = tf.keras.initializers.Constant(pretrain_embedding_matrix)\n",
    "        # the embedding layer will defaultly use GPU memory\n",
    "        # so to avoid error while trainning from optimizer which not support GPU\n",
    "        # using tf.device(\"cpu:0\") to place embedding matrix on CPU memory\n",
    "        with tf.name_scope(\"embedding\"), tf.device(\"cpu:0\"):\n",
    "            # define embedding layer where in put is array of index of word's vector in\n",
    "            # embedding matrix. the output's shape is \n",
    "            # (batch_size, sequence_length, embedding_size)\n",
    "            embed = tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_size,\n",
    "                embeddings_initializer=embed_initializers,\n",
    "                input_length=sequence_length,\n",
    "                trainable=embed_trainable,\n",
    "                name=\"embedding-layer\"\n",
    "            )(input_word_idx)\n",
    "            # define reshape layer for expand dimention of output from embedding layer to\n",
    "            # (batch_size, sequence_length, embedding_size, 1) for suit to input of\n",
    "            # 2d convolution layer which require input shape as \n",
    "            # (batch_size, weight, height, channel)\n",
    "            expand_dim_embed = tf.keras.layers.Reshape(\n",
    "                target_shape=(-1, embedding_size, 1),\n",
    "                name=\"reshape-expand-dim-layer\"\n",
    "                )(embed)\n",
    "        \n",
    "        features = list() # list for contain output of each pooling layer\n",
    "        with tf.name_scope(\"convolution\"):\n",
    "            # for loop to define each convolution layer of each size of the filter\n",
    "            for size in filter_sizes:\n",
    "                # define 2d convolution layer of n(num_filters) filters with kernel size\n",
    "                # is (size, embedding_size) strides by (1, 1) and not use padding.\n",
    "                # using ReLu as activation function also use bias. output's shape of this\n",
    "                # layer is (batch_size, sequence_length - size + 1, 1, num_filters)\n",
    "                conv = tf.keras.layers.Conv2D(\n",
    "                    filters=num_filters,\n",
    "                    kernel_size=(size, embedding_size),\n",
    "                    strides=(1, 1),\n",
    "                    padding=\"valid\",\n",
    "                    activation=\"relu\",\n",
    "                    use_bias=True,\n",
    "                    kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1),\n",
    "                    bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "                    name=f\"conv-{size}_{embedding_size}-layer\"\n",
    "                )(expand_dim_embed)\n",
    "                # define reshape layer for squeeze dimention of output from convolution layer \n",
    "                # to (batch_size, feature_map_height, num_filters)\n",
    "                reshape = tf.keras.layers.Reshape(target_shape=(-1, num_filters), name=f\"reshape-squeeze-{size}_{embedding_size}-layer\")(conv)\n",
    "                # define max-over-time pooling layer for extract max value from each \n",
    "                # feature map from convolution layers. where output is vector with \n",
    "                # shape of (batch_size, 1, num_filters)\n",
    "                pooling = tf.keras.layers.GlobalMaxPool1D(name=f\"global-max-pooling-{size}_{embedding_size}-layer\")(reshape)\n",
    "                # define flatten layer for shape output of pooling layer to \n",
    "                # (batch_size, num_filters)\n",
    "                flatten_pooling = tf.keras.layers.Flatten(name=f\"flatten-pooling-{size}_{embedding_size}-layer\")(pooling)\n",
    "                features.append(flatten_pooling)\n",
    "        \n",
    "        # define concatenate layer to concate all features for convolution & pooling \n",
    "        # process. where output shape is (batch_size, num_filters * len(filter_sizes))\n",
    "        concat = tf.keras.layers.Concatenate(name=\"concatenate-layer\")(features)\n",
    "        \n",
    "        # if dropout_rate is None, model will not include dropout layer\n",
    "        if dropout_rate is not None:\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                # define dropout layer with specified dropout_rate\n",
    "                dropout = tf.keras.layers.Dropout(rate=dropout_rate)(concat)\n",
    "                fc_input = dropout\n",
    "        else: fc_input = concat\n",
    "        \n",
    "        with tf.name_scope(\"fully-connected\"):\n",
    "            # define output layer (fully connected layer) using Softmax as activation\n",
    "            # and L2 as regularization method. where output is propability to be each \n",
    "            # class with output shape is (batch_size, num_classes).\n",
    "            output = tf.keras.layers.Dense(\n",
    "                units=num_classes,\n",
    "                activation=\"softmax\",\n",
    "                use_bias=True,\n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "                kernel_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n",
    "                bias_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n",
    "                name=\"output-layer\"\n",
    "                )(fc_input)\n",
    "        \n",
    "        # define model with all defined sequent layers\n",
    "        self.model = tf.keras.Model(inputs=input_word_idx, outputs=output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnn = TextCNN(\n",
    "                num_classes=4,\n",
    "                vocab_size=prep.embedding_shape.vocab_size,\n",
    "                embedding_size=prep.embedding_shape.embedding_size,\n",
    "                filter_sizes=[2, 3, 4],\n",
    "                num_filters=2,\n",
    "                sequence_length=None,\n",
    "                dropout_rate=0.5,\n",
    "                l2_reg_lambda=0.01,\n",
    "                seed=42,\n",
    "                pretrain_embedding_matrix=prep.word_vectors)\n",
    "\n",
    "textcnn.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model using Adam optimizer with learning rate equal to 0.001\n",
    "# categorical crossentropy as loss function and evaluation metrics are\n",
    "# precision and recall\n",
    "textcnn.model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n",
    "textcnn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you don't want to visualize model achitecture \n",
    "# ***** you don't need to run this cell just skip it.\n",
    "# You must install pydot (`pip install pydot`) and install graphviz \n",
    "# (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
    "# (e.g. for mac) $ brew install graphviz\n",
    "# (e.g. for win) $ winget install graphviz\n",
    "tf.keras.utils.plot_model(textcnn.model, \"textcnn_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = textcnn.model.fit(\n",
    "    x=padd_input,\n",
    "    y=label,\n",
    "    batch_size=None,\n",
    "    epochs=2,\n",
    "    shuffle=False\n",
    ")\n",
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnn.model.evaluate(\n",
    "    x=padd_test_input,\n",
    "    y=test_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
