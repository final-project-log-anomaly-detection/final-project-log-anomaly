{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 18:18:55.856887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class DataPreprocessor(object):\n",
    "    \n",
    "    def __init__(self, record_unknow=False) -> None:\n",
    "        if record_unknow: self.unknow_words = dict()\n",
    "    \n",
    "    @staticmethod\n",
    "    def text_cleansing(text):\n",
    "        regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "        regex_expect_words = r'[^\\w<>]+'\n",
    "        output = re.sub(regex_except_token, '', text)\n",
    "        output = re.sub(regex_expect_words, ' ', output)\n",
    "        return output\n",
    "    \n",
    "    def load_word2vec_format(self, \n",
    "                             file_path, \n",
    "                             unknow_token=None, \n",
    "                             unknow_repr=None):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            vec = dict()\n",
    "            for l in f.readlines():\n",
    "                data = l.split()\n",
    "                vec[data[0]] = np.array(data[1:], dtype=np.float32)\n",
    "            \n",
    "            f.close()\n",
    "                \n",
    "        embedding_size = vec.pop(list(vec.keys())[0])\n",
    "        if unknow_token is not None:\n",
    "            try:\n",
    "                unknow_vec = vec[unknow_token]\n",
    "            except AttributeError:\n",
    "                print(f\"there's not '{unknow_token}' in dictionary.\")\n",
    "                if unknow_repr:\n",
    "                    assert len(unknow_repr) == embedding_size, \\\n",
    "                        f\"unknown represent vector must same shape with embedding size (expected {embedding_size}, got {len(unknow_repr)})\"\n",
    "                else:\n",
    "                    unknow_repr = [ 0 for _ in range(embedding_size) ]\n",
    "                vec[unknow_token] = unknow_repr\n",
    "            self.unknow_token = unknow_token\n",
    "        else: self.unknow_token = None\n",
    "        \n",
    "        vocab_size = len(vec.keys())    \n",
    "        self.word_vectors = np.array(list(vec.values()))\n",
    "        self.words_indices = { word: i for i, word in enumerate(vec.keys()) }\n",
    "        self.embedding_shape = NamedTuple(vocab_size=vocab_size, embedding_size=embedding_size)\n",
    "    \n",
    "    def indice_encode(self, line):\n",
    "        record_unknow = hasattr(self, \"unknow_words\")\n",
    "        words = line.split()\n",
    "        encoded = list()\n",
    "        for word in words:\n",
    "            try:\n",
    "                i = self.words_indices[word]\n",
    "            except AttributeError:\n",
    "                if record_unknow:\n",
    "                    if word in self.unknow_words.keys(): self.unknow_words[word] += 1\n",
    "                    else: self.unknow_words[word] = 0\n",
    "                \n",
    "                if self.unknow_token is None: continue\n",
    "                else: i = self.words_indices[self.unknow_token]\n",
    "            encoded.append(i)\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def indice_padding(self, batch, padding_token, padding_size=0):\n",
    "        padding_idx = self.words_indices[padding_token]\n",
    "        seq_length = np.array([ len(inst) for inst in batch ])\n",
    "        max_length = seq_length.max()\n",
    "        padding_size = max(padding_size, max_length)\n",
    "        \n",
    "        padding_batch = np.array([ np.pad(inst, (0, padding_size - len(inst)), constant_values=padding_idx) for inst in batch ])\n",
    "        return padding_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNNConfig():\n",
    "    \n",
    "    def __init__(self,\n",
    "                num_classes,\n",
    "                vocab_size,\n",
    "                embedding_size, \n",
    "                filter_sizes, \n",
    "                num_filters,\n",
    "                sequence_length=None,\n",
    "                dropout_rate=None,\n",
    "                l2_reg_lambda=0.0,\n",
    "                seed=42,\n",
    "                pretrain_embedding_matrix=None\n",
    "                ) -> None:\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.seed = seed\n",
    "        self.pretrain_embedding_matrix = pretrain_embedding_matrix\n",
    "\n",
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "        num_classes,\n",
    "        vocab_size,\n",
    "        embedding_size, \n",
    "        filter_sizes, \n",
    "        num_filters,\n",
    "        sequence_length=None,\n",
    "        dropout_rate=None,\n",
    "        l2_reg_lambda=0.0,\n",
    "        seed=42,\n",
    "        pretrain_embedding_matrix=None\n",
    "        ) -> None:\n",
    "        \n",
    "        self.config = TextCNNConfig(\n",
    "            num_classes,\n",
    "            vocab_size,\n",
    "            embedding_size, \n",
    "            filter_sizes, \n",
    "            num_filters,\n",
    "            sequence_length,\n",
    "            dropout_rate,\n",
    "            l2_reg_lambda,\n",
    "            seed,\n",
    "            pretrain_embedding_matrix\n",
    "        )\n",
    "        \n",
    "        if seed is not None: tf.random.set_seed(seed)\n",
    "        input_word_idx = tf.keras.layers.Input(\n",
    "            shape=(None, sequence_length),\n",
    "            dtype=tf.dtypes.int32,\n",
    "            name=\"input-word-idx-layer\"\n",
    "        )\n",
    "        \n",
    "        embed_trainable = pretrain_embedding_matrix is None\n",
    "        if pretrain_embedding_matrix is None:\n",
    "            embed_initializers = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n",
    "        else:\n",
    "            pretrain_embedding_matrix = np.array(pretrain_embedding_matrix)\n",
    "            assert (vocab_size, embedding_size) == pretrain_embedding_matrix.shape, \\\n",
    "                f\"shape of embedding_matrix must match to vocab_size and embedding_size (expect {(vocab_size, embedding_size)}, got {pretrain_embedding_matrix.shape}).\"\n",
    "            embed_initializers = tf.keras.initializers.Constant(pretrain_embedding_matrix)\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"), tf.device(\"cpu:0\"):\n",
    "            embed = tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_size,\n",
    "                embeddings_initializer=embed_initializers,\n",
    "                input_length=sequence_length,\n",
    "                trainable=embed_trainable,\n",
    "                name=\"embedding-layer\"\n",
    "            )(input_word_idx)\n",
    "            \n",
    "            expand_dim_embed = tf.keras.layers.Reshape(\n",
    "                target_shape=(vocab_size, embedding_size, 1),\n",
    "                name=\"reshape-expand-dim-layer\"\n",
    "                )(embed)\n",
    "        \n",
    "        features = list()\n",
    "        with tf.name_scope(\"convolution\"):\n",
    "            for i in range(num_filters):\n",
    "                for size in filter_sizes:\n",
    "                    conv = tf.keras.layers.Conv2D(\n",
    "                        filters=num_filters,\n",
    "                        kernel_size=(size, embedding_size),\n",
    "                        strides=(1, 1),\n",
    "                        padding=\"valid\",\n",
    "                        activation=\"relu\",\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1),\n",
    "                        bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "                        name=f\"conv-{size}_{embedding_size}-{i}th-layer\"\n",
    "                    )(expand_dim_embed)\n",
    "                    reshape = tf.keras.layers.Reshape(target_shape=(1, -1), name=f\"reshape-2d-{size}_{embedding_size}-{i}th-layer\")(conv)\n",
    "                    pooling = tf.keras.layers.GlobalMaxPool1D(name=f\"global-max-pooling-{size}_{embedding_size}-{i}-layer\")(reshape)\n",
    "                    features.append(pooling)\n",
    "            \n",
    "        concat = tf.keras.layers.Concatenate(axis=1, name=\"concatenat-layer\")(features)\n",
    "        \n",
    "        if dropout_rate is not None:\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                dropout = tf.keras.layers.Dropout(rate=dropout_rate)(concat)\n",
    "                fc_input = dropout\n",
    "        else: fc_input = concat\n",
    "        \n",
    "        with tf.name_scope(\"fully-connected\"):\n",
    "            output = tf.keras.layers.Dense(\n",
    "                units=num_classes,\n",
    "                activation=\"softmax\",\n",
    "                use_bias=True,\n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "                kernel_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n",
    "                bias_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n",
    "                name=\"output-layer\"\n",
    "                )(fc_input)\n",
    "            \n",
    "        self.model = tf.keras.Model(inputs=input_word_idx, outputs=output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnn = TextCNN(num_classes=2,\n",
    "                vocab_size=200,\n",
    "                embedding_size=150,\n",
    "                filter_sizes=[2, 3, 4],\n",
    "                num_filters=2,\n",
    "                sequence_length=10,\n",
    "                dropout_rate=0.5,\n",
    "                l2_reg_lambda=0.01,\n",
    "                seed=42,\n",
    "                pretrain_embedding_matrix=None)\n",
    "\n",
    "textcnn.model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tfa.metrics.F1Score(num_classes=textcnn.config.num_classes, average=\"macro\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Note: If you don't want to visualize model achitecture \n",
    "# ***** you don't need to run this cell just skip it.\n",
    "# You must install pydot (`pip install pydot`) and install graphviz \n",
    "# (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
    "# (e.g. for mac) $ brew install graphviz\n",
    "# (e.g. for win) $ winget install graphviz\n",
    "tf.keras.utils.plot_model(textcnn.model, \"textcnn_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input-word-idx-layer (InputLay  [(None, None, 10)]  0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " embedding-layer (Embedding)    (None, None, 10, 15  30000       ['input-word-idx-layer[0][0]']   \n",
      "                                0)                                                                \n",
      "                                                                                                  \n",
      " reshape-expand-dim-layer (Resh  (None, 200, 150, 1)  0          ['embedding-layer[0][0]']        \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv-2_150-0th-layer (Conv2D)  (None, 199, 1, 2)    602         ['reshape-expand-dim-layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv-3_150-0th-layer (Conv2D)  (None, 198, 1, 2)    902         ['reshape-expand-dim-layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv-4_150-0th-layer (Conv2D)  (None, 197, 1, 2)    1202        ['reshape-expand-dim-layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv-2_150-1th-layer (Conv2D)  (None, 199, 1, 2)    602         ['reshape-expand-dim-layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv-3_150-1th-layer (Conv2D)  (None, 198, 1, 2)    902         ['reshape-expand-dim-layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv-4_150-1th-layer (Conv2D)  (None, 197, 1, 2)    1202        ['reshape-expand-dim-layer[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " reshape-2d-2_150-0th-layer (Re  (None, 1, 398)      0           ['conv-2_150-0th-layer[0][0]']   \n",
      " shape)                                                                                           \n",
      "                                                                                                  \n",
      " reshape-2d-3_150-0th-layer (Re  (None, 1, 396)      0           ['conv-3_150-0th-layer[0][0]']   \n",
      " shape)                                                                                           \n",
      "                                                                                                  \n",
      " reshape-2d-4_150-0th-layer (Re  (None, 1, 394)      0           ['conv-4_150-0th-layer[0][0]']   \n",
      " shape)                                                                                           \n",
      "                                                                                                  \n",
      " reshape-2d-2_150-1th-layer (Re  (None, 1, 398)      0           ['conv-2_150-1th-layer[0][0]']   \n",
      " shape)                                                                                           \n",
      "                                                                                                  \n",
      " reshape-2d-3_150-1th-layer (Re  (None, 1, 396)      0           ['conv-3_150-1th-layer[0][0]']   \n",
      " shape)                                                                                           \n",
      "                                                                                                  \n",
      " reshape-2d-4_150-1th-layer (Re  (None, 1, 394)      0           ['conv-4_150-1th-layer[0][0]']   \n",
      " shape)                                                                                           \n",
      "                                                                                                  \n",
      " global-max-pooling-2_150-0-lay  (None, 398)         0           ['reshape-2d-2_150-0th-layer[0][0\n",
      " er (GlobalMaxPooling1D)                                         ]']                              \n",
      "                                                                                                  \n",
      " global-max-pooling-3_150-0-lay  (None, 396)         0           ['reshape-2d-3_150-0th-layer[0][0\n",
      " er (GlobalMaxPooling1D)                                         ]']                              \n",
      "                                                                                                  \n",
      " global-max-pooling-4_150-0-lay  (None, 394)         0           ['reshape-2d-4_150-0th-layer[0][0\n",
      " er (GlobalMaxPooling1D)                                         ]']                              \n",
      "                                                                                                  \n",
      " global-max-pooling-2_150-1-lay  (None, 398)         0           ['reshape-2d-2_150-1th-layer[0][0\n",
      " er (GlobalMaxPooling1D)                                         ]']                              \n",
      "                                                                                                  \n",
      " global-max-pooling-3_150-1-lay  (None, 396)         0           ['reshape-2d-3_150-1th-layer[0][0\n",
      " er (GlobalMaxPooling1D)                                         ]']                              \n",
      "                                                                                                  \n",
      " global-max-pooling-4_150-1-lay  (None, 394)         0           ['reshape-2d-4_150-1th-layer[0][0\n",
      " er (GlobalMaxPooling1D)                                         ]']                              \n",
      "                                                                                                  \n",
      " concatenat-layer (Concatenate)  (None, 2376)        0           ['global-max-pooling-2_150-0-laye\n",
      "                                                                 r[0][0]',                        \n",
      "                                                                  'global-max-pooling-3_150-0-laye\n",
      "                                                                 r[0][0]',                        \n",
      "                                                                  'global-max-pooling-4_150-0-laye\n",
      "                                                                 r[0][0]',                        \n",
      "                                                                  'global-max-pooling-2_150-1-laye\n",
      "                                                                 r[0][0]',                        \n",
      "                                                                  'global-max-pooling-3_150-1-laye\n",
      "                                                                 r[0][0]',                        \n",
      "                                                                  'global-max-pooling-4_150-1-laye\n",
      "                                                                 r[0][0]']                        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 2376)         0           ['concatenat-layer[0][0]']       \n",
      "                                                                                                  \n",
      " output-layer (Dense)           (None, 2)            4754        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,166\n",
      "Trainable params: 40,166\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textcnn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb13736d41bd87c4a6da44de2a53cc3b2c3a6ec16bd51d52b3766a818a83a02f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
