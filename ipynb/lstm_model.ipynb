{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # tensorflow v2.11.0\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from typing import NamedTuple\n",
    "\n",
    "EmbeddingShape = NamedTuple(\"EmbeddingShape\", [(\"vocab_size\", int), (\"embedding_size\", int)])\n",
    "'''(NamedTuple) class for embedding shape contain vocab_size and embedding_size'''\n",
    "\n",
    "class LogPreprocessor(object):\n",
    "    \"\"\"LogPreprocessor\n",
    "    class for preprocessing data before feed to model\n",
    "    \n",
    "    Args:\n",
    "        record_unknow (bool): whether recording unknown word from preprocessing process or not. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, record_unknow=False) -> None:\n",
    "        if record_unknow: \n",
    "            self.unknow_words = dict()\n",
    "            '''dictionary for counting ocurred unknow word from preprocessing process where keys is word and value is occuring number'''\n",
    "    \n",
    "    @staticmethod\n",
    "    def text_cleansing(text):\n",
    "        \"\"\"(static method) text_cleansing.\n",
    "        method for cleansing log text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): log text to cleansing special character.\n",
    "\n",
    "        Returns:\n",
    "            str: cleansed log text.\n",
    "        \"\"\"\n",
    "        regex_except_token = r'\\B(?!<\\w+>\\B)[^\\w\\s]'\n",
    "        regex_expect_words = r'[^\\w<>]+'\n",
    "        output = re.sub(regex_except_token, '', text)\n",
    "        output = re.sub(regex_expect_words, ' ', output)\n",
    "        return output\n",
    "    \n",
    "    def load_word2vec_format(self, \n",
    "                             file_path, \n",
    "                             unknow_token=None, \n",
    "                             unknow_repr=None):\n",
    "        \"\"\"(instance method) load_word2vec_format\n",
    "        loading word2vec format file to extract embeadding metrix\n",
    "\n",
    "        Args:\n",
    "            file_path (str): file's path to word2vec format file (.txt)\n",
    "            unknow_token (str, optional): word for represent unknown word (e.g. '<OOV>'). \n",
    "            Defaults to None.\n",
    "            unknow_repr (_ArrayLike, optional): word vector for represent unknown word. \n",
    "            if 'unknow_repr' is not set but 'unknow_token' is set \n",
    "            it will use zero vector as unknow represent vector. Defaults to None.\n",
    "        \"\"\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            vec = dict()\n",
    "            for l in f.readlines():\n",
    "                data = list(filter(None, re.split(\" +\", l)))\n",
    "                vec[data[0]] = np.array(data[1:], dtype=np.float32)\n",
    "            \n",
    "            f.close()\n",
    "                \n",
    "        embedding_size = int(vec.pop(list(vec.keys())[0])[0])\n",
    "        if unknow_token is not None:\n",
    "            try:\n",
    "                unknow_vec = vec[unknow_token]\n",
    "            except AttributeError:\n",
    "                print(f\"there's not '{unknow_token}' in dictionary.\")\n",
    "                if unknow_repr:\n",
    "                    assert len(unknow_repr) == embedding_size, \\\n",
    "                        f\"unknown represent vector must same shape with embedding size (expected {embedding_size}, got {len(unknow_repr)})\"\n",
    "                else:\n",
    "                    unknow_repr = [ 0 for _ in range(embedding_size) ]\n",
    "                vec[unknow_token] = unknow_repr\n",
    "            self.unknow_token = unknow_token\n",
    "        else: \n",
    "            self.unknow_token = None\n",
    "        \n",
    "        vocab_size = len(vec.keys())    \n",
    "        self.word_vectors = np.array(list(vec.values()))\n",
    "        self.words_indices = { word: i for i, word in enumerate(vec.keys()) }\n",
    "        self.embedding_shape = EmbeddingShape(vocab_size, embedding_size)\n",
    "    \n",
    "    def indice_encode(self, line):\n",
    "        \"\"\"(instance method) indice_encode.\n",
    "        encoding string words in log line to index number.\n",
    "\n",
    "        Args:\n",
    "            line (_ArrayLike[str]): array of tokenized string line.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: list contains index number.\n",
    "        \"\"\"\n",
    "        record_unknow = hasattr(self, \"unknow_words\")\n",
    "        encoded = list()\n",
    "        \n",
    "        \n",
    "        for word in line:\n",
    "            try:\n",
    "                i = self.words_indices[word]\n",
    "            except KeyError:\n",
    "                if record_unknow:\n",
    "                    if word in self.unknow_words.keys(): self.unknow_words[word] += 1\n",
    "                    else: self.unknow_words[word] = 0\n",
    "                \n",
    "                if self.unknow_token is None: continue\n",
    "                else: i = self.words_indices[self.unknow_token]\n",
    "            encoded.append(i)\n",
    "\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def insert_new_word(self, word, vector, index=-1):\n",
    "        \"\"\"(instance method) insert_new_word\n",
    "        insert a new word and vector to word dictionary at specified index\n",
    "        \n",
    "        Args:\n",
    "            word (str): new word to insert\n",
    "            vector (_ArrayLike[Float]): new vector to insert\n",
    "            index (int, optional): insert at index if negative index will insert at (vocab_size + index + 1). Defaults to -1.\n",
    "        \"\"\"\n",
    "        if index < 0: index = self.embedding_shape.vocab_size + index + 1\n",
    "        assert word not in self.words_indices.keys(), \\\n",
    "            f\"there is already exist input word in vocab at {self.words_indices[word]} index\"\n",
    "        assert np.ndim(vector) == 1, \\\n",
    "            f\"expect 1 dim vector as a input but got {np.ndim(vector)}.\"\n",
    "        assert len(vector) == self.embedding_shape.embedding_size, \\\n",
    "            f\"insert vector's shape must match to embedding size. (got {np.shape(vector)})\"\n",
    "            \n",
    "        new_word_vectors = np.insert(self.word_vectors, index, vector, axis=0)\n",
    "        new_words_indices = { k: v if v < index else v + 1 for k, v in self.words_indices.items() }\n",
    "        new_words_indices[word] = index\n",
    "        new_embedding_shape = EmbeddingShape(self.embedding_shape.vocab_size + 1, self.embedding_shape.embedding_size)\n",
    "        \n",
    "        self.word_vectors = new_word_vectors\n",
    "        self.words_indices = new_words_indices\n",
    "        self.embedding_shape = new_embedding_shape\n",
    "    \n",
    "    def indice_padding(self, batch, padding_token, padding_size=0):\n",
    "        \"\"\"(instance method) indice_padding.\n",
    "        padding tokenized array in batch by index number of padding_token.\n",
    "        \n",
    "        Args:\n",
    "            batch (_ArrayLike[_ArrayLike[int]]): data batch of array of index number.\n",
    "            padding_token (str): word for represent padding token (e.g. \"<PADDING>\") where padding token must in dictionary.\n",
    "            padding_size (int, optional): size of output. if 'padding_size' is less than longest line in batch it will set to length of longest line is batch. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            NDArray[NDArray[int]]: array of padded batch data.\n",
    "        \"\"\"\n",
    "        padding_idx = self.words_indices[padding_token]\n",
    "        seq_length = np.array([ len(inst) for inst in batch ])\n",
    "        max_length = seq_length.max()\n",
    "        padding_size = max(padding_size, max_length)\n",
    "        \n",
    "        padding_batch = np.array([ np.pad(inst, (0, padding_size - len(inst)), constant_values=padding_idx) for inst in batch ])\n",
    "        return padding_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_label(y, classes=None):\n",
    "        \"\"\"(static method) build_label\n",
    "        build labels array using one-hot encoding\n",
    "\n",
    "        Args:\n",
    "            y (_ArrayLike): input labels\n",
    "            classes (_ArrayLike, optional): array contains all classes. if None, it will set to unique label of input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            _Array[_Array[int]]: array of one-hot label where shape is (input_size, num_classes)\n",
    "        \"\"\"\n",
    "        if classes is None:\n",
    "            classes = np.unique(y)\n",
    "            \n",
    "        encoder = OneHotEncoder(categories=[classes])\n",
    "        return encoder.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = LogPreprocessor()\n",
    "# extract word2vec format file to embedding matrix and words indice\n",
    "# change this path for select another embedding method\n",
    "prep.load_word2vec_format(\"../BGL_word2Vec-fine-tune-embedder.txt\")\n",
    "# insert \"<PAD>\" at first index of embedding matrix with zeros vector\n",
    "# if padding token is represented as zeros vector, unknown token must be represented\n",
    "# as non-zeros vector. (that mean if you set unknow_token for load_word2vec_format you\n",
    "# have to set unknow_repr too or using non-zero vector for represent padding token).\n",
    "prep.insert_new_word(\"<PAD>\", np.zeros(prep.embedding_shape.embedding_size), index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load processed trainset of structured log and template log data\n",
    "prep_train_struc_log = pd.read_pickle(\"../data_preprocess/processed_type2/BGL_preprocessed_type2/train_set.pkl\")\n",
    "prep_train_templ_log = pd.read_pickle(\"../data_preprocess/processed_type2/BGL_preprocessed_type2/template_train_set.pkl\")\n",
    "\n",
    "# encode token to index of embedding matrix\n",
    "prep_train_struc_log[\"Token_Indice_encoded\"] = prep_train_struc_log.Token.map(prep.indice_encode)\n",
    "# padding token with \"<PAD>\"\n",
    "padd_input = prep.indice_padding(prep_train_struc_log[\"Token_Indice_encoded\"], \"<PAD>\")\n",
    "# build label array to one hot format\n",
    "label = prep.build_label(prep_train_struc_log[\"Label\"].to_numpy().reshape((-1, 1)), classes=[0, 1])\n",
    "# you also can use this preprocessing pipeline for testset\n",
    "\n",
    "prep_test_struc_log = pd.read_pickle(\"../data_preprocess/processed_type2/BGL_preprocessed_type2/test_set.pkl\")\n",
    "prep_test_templ_log = pd.read_pickle(\"../data_preprocess/processed_type2/BGL_preprocessed_type2/template_test_set.pkl\")\n",
    "\n",
    "prep_test_struc_log[\"Token_Indice_encoded\"] = prep_test_struc_log.Token.map(prep.indice_encode)\n",
    "\n",
    "padd_test_input = prep.indice_padding(prep_test_struc_log[\"Token_Indice_encoded\"], \"<PAD>\")\n",
    "\n",
    "test_label = prep.build_label(prep_test_struc_log[\"Label\"].to_numpy().reshape((-1, 1)), classes=[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTMConfig():\n",
    "    def __init__(self,\n",
    "                num_classes,\n",
    "                vocab_size,\n",
    "                embedding_size, \n",
    "                filter_sizes, \n",
    "                num_filters,\n",
    "                sequence_length=None,\n",
    "                dropout_rate=None,\n",
    "                l2_reg_lambda=0.0,\n",
    "                seed=42,\n",
    "                pretrain_embedding_matrix=None\n",
    "                ) -> None:\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.seed = seed\n",
    "        self.pretrain_embedding_matrix = pretrain_embedding_matrix\n",
    "       \n",
    "    def __repr__(self):\n",
    "        return \"TextLSTMConfig:\\n\" +\\\n",
    "            f\"num_classes:      {self.num_classes}\\n\" +\\\n",
    "            f\"vocab_sizes:      {self.vocab_size}\\n\" +\\\n",
    "            f\"embedding_size:   {self.embedding_size}\\n\" +\\\n",
    "            f\"filter_sizes:     {self.filter_sizes}\\n\" +\\\n",
    "            f\"num_filters:      {self.num_filters}\\n\" +\\\n",
    "            f\"sequence_length:  {self.sequence_length}\\n\" +\\\n",
    "            f\"dropout_rate:     {self.dropout_rate}\\n\" +\\\n",
    "            f\"l2_reg_lambda:    {self.l2_reg_lambda}\\n\" +\\\n",
    "            f\"seed:             {self.seed}\\n\" +\\\n",
    "            f\"pretrain_embedding_matrix's shape: {np.shape(self.pretrain_embedding_matrix) if self.pretrain_embedding_matrix is not None else 'None'}\"\n",
    "            \n",
    "class TextLSTM(object):\n",
    "    def __init__(self,\n",
    "        num_classes,\n",
    "        vocab_size,\n",
    "        embedding_size, \n",
    "        filter_sizes, \n",
    "        num_filters,\n",
    "        sequence_length=None,\n",
    "        dropout_rate=None,\n",
    "        l2_reg_lambda=0.0,\n",
    "        seed=42,\n",
    "        pretrain_embedding_matrix=None\n",
    "        ) -> None:\n",
    "        \n",
    "        self.config = TextLSTMConfig(\n",
    "            num_classes,\n",
    "            vocab_size,\n",
    "            embedding_size, \n",
    "            filter_sizes, \n",
    "            num_filters,\n",
    "            sequence_length,\n",
    "            dropout_rate,\n",
    "            l2_reg_lambda,\n",
    "            seed,\n",
    "            pretrain_embedding_matrix\n",
    "        )\n",
    "        \n",
    "        if seed is not None: tf.random.set_seed(seed)\n",
    "        \n",
    "        input_word_idx = tf.keras.layers.Input(\n",
    "            shape=(sequence_length,),\n",
    "            dtype=tf.dtypes.int32,\n",
    "            name=\"input-word-idx-layer\"\n",
    "        )\n",
    "        \n",
    "        embed_trainable = pretrain_embedding_matrix is None\n",
    "        if pretrain_embedding_matrix is None:\n",
    "            embed_initializers = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n",
    "        else:\n",
    "            pretrain_embedding_matrix = np.array(pretrain_embedding_matrix)\n",
    "            assert (vocab_size, embedding_size) == pretrain_embedding_matrix.shape, \\\n",
    "                f\"shape of embedding_matrix must match to vocab_size and embedding_size (expect {(vocab_size, embedding_size)}, got {pretrain_embedding_matrix.shape}).\"\n",
    "            embed_initializers = tf.keras.initializers.Constant(pretrain_embedding_matrix)\n",
    "            \n",
    "        with tf.name_scope(\"embedding\"), tf.device(\"cpu:0\"):\n",
    "            # define embedding layer where in put is array of index of word's vector in\n",
    "            # embedding matrix. the output's shape is \n",
    "            # (batch_size, sequence_length, embedding_size)\n",
    "            embed = tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_size,\n",
    "                embeddings_initializer=embed_initializers,\n",
    "                input_length=sequence_length,\n",
    "                trainable=embed_trainable,\n",
    "                name=\"embedding-layer\"\n",
    "            )(input_word_idx)\n",
    "\n",
    "        with tf.name_scope(\"convolution\"):\n",
    "            lstm = tf.keras.layers.LSTM()(embed)     \n",
    "        \n",
    "        if dropout_rate is not None:\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                # define dropout layer with specified dropout_rate\n",
    "                dropout = tf.keras.layers.Dropout(rate=dropout_rate)(lstm)\n",
    "                fc_input = dropout\n",
    "        else: fc_input = lstm \n",
    "        \n",
    "        with tf.name_scope(\"fully-connected\"):\n",
    "            # define output layer (fully connected layer) using Softmax as activation\n",
    "            # and L2 as regularization method. where output is propability to be each \n",
    "            # class with output shape is (batch_size, num_classes).\n",
    "            output = tf.keras.layers.Dense(\n",
    "                units=num_classes,\n",
    "                activation=\"softmax\",\n",
    "                use_bias=True,\n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                bias_initializer=tf.keras.initializers.Constant(0.1),\n",
    "                kernel_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n",
    "                bias_regularizer=tf.keras.regularizers.L2(l2=l2_reg_lambda),\n",
    "                name=\"output-layer\"\n",
    "                )(fc_input)\n",
    "        \n",
    "        # define model with all defined sequent layers\n",
    "        self.model = tf.keras.Model(inputs=input_word_idx, outputs=output)    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnn = TextLSTM(\n",
    "                num_classes=2,\n",
    "                vocab_size=prep.embedding_shape.vocab_size,\n",
    "                embedding_size=prep.embedding_shape.embedding_size,\n",
    "                filter_sizes=[2, 3, 4],\n",
    "                num_filters=2,\n",
    "                sequence_length=None,\n",
    "                dropout_rate=0.5,\n",
    "                l2_reg_lambda=0.01,\n",
    "                seed=42,\n",
    "                pretrain_embedding_matrix=prep.word_vectors)\n",
    "\n",
    "textcnn.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
